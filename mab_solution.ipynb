{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Part 1: Multi-Armed Bandit Algorithms for Profit-Aware Product Recommendation\n",
        "This module implements various MAB strategies for e-commerce product recommendations.\n",
        "\n",
        "Author: DRL Assignment Solution\n",
        "Date: December 2025\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class MultiArmedBandit:\n",
        "    \"\"\"\n",
        "    Multi-Armed Bandit base class for product recommendation.\n",
        "    Handles the environment with 6 products (arms) and user sessions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, n_arms: int = 6):\n",
        "        \"\"\"\n",
        "        Initialize the MAB environment.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame containing user sessions with product revenues and costs\n",
        "            n_arms: Number of products/arms (default: 6)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.n_arms = n_arms\n",
        "        self.n_users = len(data)\n",
        "        self.current_user = 0\n",
        "        self.net_rewards = self._compute_net_rewards()\n",
        "\n",
        "    def _compute_net_rewards(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute net rewards (profit) for all products across all users.\n",
        "        NetReward = Revenue - Cost\n",
        "\n",
        "        Returns:\n",
        "            Array of shape (n_users, n_arms) with net rewards\n",
        "        \"\"\"\n",
        "        net_rewards = np.zeros((self.n_users, self.n_arms))\n",
        "\n",
        "        for arm in range(self.n_arms):\n",
        "            product_col = f'Product{arm + 1}'\n",
        "            cost_col = f'cost{arm + 1}'\n",
        "            net_rewards[:, arm] = self.data[product_col].values - self.data[cost_col].values\n",
        "\n",
        "        return net_rewards\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to start a new simulation.\"\"\"\n",
        "        self.current_user = 0\n",
        "\n",
        "    def step(self, action: int) -> Tuple[float, bool]:\n",
        "        \"\"\"\n",
        "        Take an action (select a product) and observe the reward.\n",
        "\n",
        "        Args:\n",
        "            action: Product index (0-5)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (reward, done)\n",
        "        \"\"\"\n",
        "        if self.current_user >= self.n_users:\n",
        "            return 0.0, True\n",
        "\n",
        "        reward = self.net_rewards[self.current_user, action]\n",
        "        self.current_user += 1\n",
        "        done = self.current_user >= self.n_users\n",
        "\n",
        "        return reward, done\n",
        "\n",
        "    def get_optimal_arm(self, user_idx: int) -> int:\n",
        "        \"\"\"\n",
        "        Get the optimal arm for a specific user.\n",
        "\n",
        "        Args:\n",
        "            user_idx: User index\n",
        "\n",
        "        Returns:\n",
        "            Arm index with highest reward\n",
        "        \"\"\"\n",
        "        return np.argmax(self.net_rewards[user_idx])\n",
        "\n",
        "\n",
        "class RandomPolicy:\n",
        "    \"\"\"\n",
        "    Random recommendation policy: randomly select one product for each user.\n",
        "    This represents the current organizational policy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_arms: int):\n",
        "        \"\"\"\n",
        "        Initialize random policy.\n",
        "\n",
        "        Args:\n",
        "            n_arms: Number of products/arms\n",
        "        \"\"\"\n",
        "        self.n_arms = n_arms\n",
        "\n",
        "    def select_action(self) -> int:\n",
        "        \"\"\"\n",
        "        Select a random action.\n",
        "\n",
        "        Returns:\n",
        "            Random arm index\n",
        "        \"\"\"\n",
        "        return np.random.randint(0, self.n_arms)\n",
        "\n",
        "    def update(self, action: int, reward: float):\n",
        "        \"\"\"Update policy (no learning in random policy).\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class GreedyPolicy:\n",
        "    \"\"\"\n",
        "    Greedy policy: try each product a few times, then always select the best one.\n",
        "    This can lead to premature lock-in on a suboptimal product.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_arms: int, exploration_rounds: int = 50):\n",
        "        \"\"\"\n",
        "        Initialize greedy policy.\n",
        "\n",
        "        Args:\n",
        "            n_arms: Number of products/arms\n",
        "            exploration_rounds: Number of initial rounds to explore each arm\n",
        "        \"\"\"\n",
        "        self.n_arms = n_arms\n",
        "        self.exploration_rounds = exploration_rounds\n",
        "        self.counts = np.zeros(n_arms)\n",
        "        self.values = np.zeros(n_arms)\n",
        "        self.t = 0\n",
        "\n",
        "    def select_action(self) -> int:\n",
        "        \"\"\"\n",
        "        Select action using greedy strategy.\n",
        "\n",
        "        Returns:\n",
        "            Arm index\n",
        "        \"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        if self.t <= self.exploration_rounds:\n",
        "            return (self.t - 1) % self.n_arms\n",
        "        else:\n",
        "            return np.argmax(self.values)\n",
        "\n",
        "    def update(self, action: int, reward: float):\n",
        "        \"\"\"\n",
        "        Update value estimates.\n",
        "\n",
        "        Args:\n",
        "            action: Arm that was selected\n",
        "            reward: Observed reward\n",
        "        \"\"\"\n",
        "        self.counts[action] += 1\n",
        "        n = self.counts[action]\n",
        "        value = self.values[action]\n",
        "        self.values[action] = value + (reward - value) / n\n",
        "\n",
        "\n",
        "class EpsilonGreedyPolicy:\n",
        "    \"\"\"\n",
        "    Epsilon-Greedy policy: mostly exploit the best product, but occasionally explore others.\n",
        "    This balances exploration and exploitation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_arms: int, epsilon: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initialize epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            n_arms: Number of products/arms\n",
        "            epsilon: Exploration probability (0-1)\n",
        "        \"\"\"\n",
        "        self.n_arms = n_arms\n",
        "        self.epsilon = epsilon\n",
        "        self.counts = np.zeros(n_arms)\n",
        "        self.values = np.zeros(n_arms)\n",
        "\n",
        "    def select_action(self) -> int:\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy strategy.\n",
        "\n",
        "        Returns:\n",
        "            Arm index\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(0, self.n_arms)\n",
        "        else:\n",
        "            return np.argmax(self.values)\n",
        "\n",
        "    def update(self, action: int, reward: float):\n",
        "        \"\"\"\n",
        "        Update value estimates.\n",
        "\n",
        "        Args:\n",
        "            action: Arm that was selected\n",
        "            reward: Observed reward\n",
        "        \"\"\"\n",
        "        self.counts[action] += 1\n",
        "        n = self.counts[action]\n",
        "        value = self.values[action]\n",
        "        self.values[action] = value + (reward - value) / n\n",
        "\n",
        "\n",
        "class UCBPolicy:\n",
        "    \"\"\"\n",
        "    Upper Confidence Bound (UCB) policy: explore products with uncertain performance.\n",
        "    This addresses the exploration-exploitation tradeoff more intelligently.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_arms: int, c: float = 2.0):\n",
        "        \"\"\"\n",
        "        Initialize UCB policy.\n",
        "\n",
        "        Args:\n",
        "            n_arms: Number of products/arms\n",
        "            c: Exploration parameter\n",
        "        \"\"\"\n",
        "        self.n_arms = n_arms\n",
        "        self.c = c\n",
        "        self.counts = np.zeros(n_arms)\n",
        "        self.values = np.zeros(n_arms)\n",
        "        self.t = 0\n",
        "\n",
        "    def select_action(self) -> int:\n",
        "        \"\"\"\n",
        "        Select action using UCB strategy.\n",
        "\n",
        "        Returns:\n",
        "            Arm index\n",
        "        \"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        for arm in range(self.n_arms):\n",
        "            if self.counts[arm] == 0:\n",
        "                return arm\n",
        "\n",
        "        ucb_values = np.zeros(self.n_arms)\n",
        "        for arm in range(self.n_arms):\n",
        "            bonus = self.c * np.sqrt(np.log(self.t) / self.counts[arm])\n",
        "            ucb_values[arm] = self.values[arm] + bonus\n",
        "\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "    def update(self, action: int, reward: float):\n",
        "        \"\"\"\n",
        "        Update value estimates.\n",
        "\n",
        "        Args:\n",
        "            action: Arm that was selected\n",
        "            reward: Observed reward\n",
        "        \"\"\"\n",
        "        self.counts[action] += 1\n",
        "        n = self.counts[action]\n",
        "        value = self.values[action]\n",
        "        self.values[action] = value + (reward - value) / n\n",
        "\n",
        "\n",
        "def run_simulation(env: MultiArmedBandit, policy, n_rounds: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Run a simulation with a given policy.\n",
        "\n",
        "    Args:\n",
        "        env: MAB environment\n",
        "        policy: Policy to use\n",
        "        n_rounds: Number of rounds to simulate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with simulation results\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    cumulative_rewards = []\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(n_rounds):\n",
        "        action = policy.select_action()\n",
        "        reward, done = env.step(action)\n",
        "\n",
        "        policy.update(action, reward)\n",
        "\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        total_reward += reward\n",
        "        cumulative_rewards.append(total_reward)\n",
        "\n",
        "        if done:\n",
        "            env.reset()\n",
        "\n",
        "    return {\n",
        "        'rewards': np.array(rewards),\n",
        "        'actions': np.array(actions),\n",
        "        'cumulative_rewards': np.array(cumulative_rewards),\n",
        "        'total_reward': total_reward,\n",
        "        'average_reward': total_reward / n_rounds,\n",
        "        'action_counts': np.bincount(actions, minlength=env.n_arms)\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_net_rewards(net_rewards: np.ndarray) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyze net rewards to identify best and worst products.\n",
        "\n",
        "    Args:\n",
        "        net_rewards: Array of net rewards (n_users, n_arms)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with analysis results\n",
        "    \"\"\"\n",
        "    mean_rewards = net_rewards.mean(axis=0)\n",
        "    median_rewards = np.median(net_rewards, axis=0)\n",
        "    positive_counts = (net_rewards > 0).sum(axis=0)\n",
        "\n",
        "    best_product = np.argmax(mean_rewards)\n",
        "    worst_product = np.argmin(mean_rewards)\n",
        "\n",
        "    return {\n",
        "        'mean_rewards': mean_rewards,\n",
        "        'median_rewards': median_rewards,\n",
        "        'positive_counts': positive_counts,\n",
        "        'best_product': best_product + 1,\n",
        "        'worst_product': worst_product + 1,\n",
        "        'best_mean_reward': mean_rewards[best_product],\n",
        "        'worst_mean_reward': mean_rewards[worst_product]\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_cumulative_rewards(results_dict: Dict[str, Dict], title: str = \"Cumulative Net Profit Comparison\"):\n",
        "    \"\"\"\n",
        "    Plot cumulative rewards for multiple strategies.\n",
        "\n",
        "    Args:\n",
        "        results_dict: Dictionary mapping strategy names to results\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for strategy_name, results in results_dict.items():\n",
        "        plt.plot(results['cumulative_rewards'], label=strategy_name, linewidth=2)\n",
        "\n",
        "    plt.xlabel('Round', fontsize=12)\n",
        "    plt.ylabel('Cumulative Net Profit', fontsize=12)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_reward_distribution(net_rewards: np.ndarray):\n",
        "    \"\"\"\n",
        "    Plot distribution of net rewards for all products.\n",
        "\n",
        "    Args:\n",
        "        net_rewards: Array of net rewards (n_users, n_arms)\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(6):\n",
        "        axes[i].hist(net_rewards[:, i], bins=30, alpha=0.7, color=f'C{i}', edgecolor='black')\n",
        "        axes[i].axvline(net_rewards[:, i].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "        axes[i].set_title(f'Product {i+1}', fontweight='bold')\n",
        "        axes[i].set_xlabel('Net Reward')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run all experiments and answer assignment questions.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"PART 1: MULTI-ARMED BANDIT FOR PRODUCT RECOMMENDATION\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/SahithiSiripuram/drl/main/Dataset_Product_Recommendation.csv\"\n",
        "    print(f\"Loading dataset from: {url}\")\n",
        "\n",
        "    try:\n",
        "        data = pd.read_csv(url)\n",
        "        print(f\"Dataset loaded successfully! Shape: {data.shape}\")\n",
        "        print(f\"Columns: {list(data.columns)}\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        print(\"Creating synthetic dataset for demonstration...\")\n",
        "        n_users = 498\n",
        "        data = pd.DataFrame()\n",
        "        data['UserID'] = range(1, n_users + 1)\n",
        "\n",
        "        np.random.seed(42)\n",
        "        for i in range(1, 7):\n",
        "            data[f'Product{i}'] = np.random.gamma(5, 10, n_users) + np.random.normal(0, 5, n_users)\n",
        "            data[f'cost{i}'] = np.random.gamma(3, 5, n_users) + np.random.normal(0, 2, n_users)\n",
        "        print(\"Synthetic dataset created!\")\n",
        "        print()\n",
        "\n",
        "    env = MultiArmedBandit(data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Q1: NET REWARD ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    analysis = analyze_net_rewards(env.net_rewards)\n",
        "    print(f\"\\nAverage Net Rewards by Product:\")\n",
        "    for i, mean_rew in enumerate(analysis['mean_rewards']):\n",
        "        print(f\"  Product {i+1}: ${mean_rew:.2f}\")\n",
        "\n",
        "    print(f\"\\nBest Product: Product {analysis['best_product']} (avg: ${analysis['best_mean_reward']:.2f})\")\n",
        "    print(f\"Worst Product: Product {analysis['worst_product']} (avg: ${analysis['worst_mean_reward']:.2f})\")\n",
        "\n",
        "    print(f\"\\nPositive Profit Sessions by Product:\")\n",
        "    for i, count in enumerate(analysis['positive_counts']):\n",
        "        pct = (count / env.n_users) * 100\n",
        "        print(f\"  Product {i+1}: {count}/{env.n_users} ({pct:.1f}%)\")\n",
        "\n",
        "    plot_reward_distribution(env.net_rewards)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Q2: RANDOM POLICY SIMULATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    n_rounds = 500\n",
        "    print(f\"\\nSimulating {n_rounds} rounds with Random Policy...\")\n",
        "\n",
        "    random_policy = RandomPolicy(env.n_arms)\n",
        "    random_results = run_simulation(env, random_policy, n_rounds)\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Total Profit: ${random_results['total_reward']:.2f}\")\n",
        "    print(f\"  Average Profit per Round: ${random_results['average_reward']:.2f}\")\n",
        "    print(f\"\\n  Product Selection Counts:\")\n",
        "    for i, count in enumerate(random_results['action_counts']):\n",
        "        pct = (count / n_rounds) * 100\n",
        "        print(f\"    Product {i+1}: {count} times ({pct:.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Q3: GREEDY POLICY SIMULATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nSimulating {n_rounds} rounds with Greedy Policy...\")\n",
        "    print(\"Strategy: Try each product a few times, then always pick the best\")\n",
        "\n",
        "    greedy_policy = GreedyPolicy(env.n_arms, exploration_rounds=60)\n",
        "    greedy_results = run_simulation(env, greedy_policy, n_rounds)\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Total Profit: ${greedy_results['total_reward']:.2f}\")\n",
        "    print(f\"  Average Profit per Round: ${greedy_results['average_reward']:.2f}\")\n",
        "    print(f\"\\n  Product Selection Counts:\")\n",
        "    for i, count in enumerate(greedy_results['action_counts']):\n",
        "        pct = (count / n_rounds) * 100\n",
        "        print(f\"    Product {i+1}: {count} times ({pct:.1f}%)\")\n",
        "\n",
        "    most_chosen = np.argmax(greedy_results['action_counts'])\n",
        "    print(f\"\\n  Most Frequently Chosen: Product {most_chosen + 1}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Q4: EPSILON-GREEDY WITH DIFFERENT EXPLORATION RATES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    exploration_rates = [0.02, 0.10, 0.25]\n",
        "    epsilon_results = {}\n",
        "\n",
        "    for epsilon in exploration_rates:\n",
        "        print(f\"\\nSimulating with {epsilon*100:.0f}% exploration rate...\")\n",
        "        policy = EpsilonGreedyPolicy(env.n_arms, epsilon=epsilon)\n",
        "        results = run_simulation(env, policy, n_rounds)\n",
        "        epsilon_results[f'\u03b5={epsilon*100:.0f}%'] = results\n",
        "\n",
        "        print(f\"  Total Profit: ${results['total_reward']:.2f}\")\n",
        "        print(f\"  Average Profit per Round: ${results['average_reward']:.2f}\")\n",
        "        print(f\"  Product Selection Counts:\")\n",
        "        for i, count in enumerate(results['action_counts']):\n",
        "            pct = (count / n_rounds) * 100\n",
        "            print(f\"    Product {i+1}: {count} times ({pct:.1f}%)\")\n",
        "\n",
        "    best_epsilon = max(epsilon_results.items(), key=lambda x: x[1]['total_reward'])\n",
        "    print(f\"\\nBest Exploration Rate: {best_epsilon[0]} with total profit ${best_epsilon[1]['total_reward']:.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Q5: UCB POLICY - INTELLIGENT EXPLORATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nSimulating {n_rounds} rounds with UCB Policy...\")\n",
        "    print(\"Strategy: Explore products with uncertain performance\")\n",
        "\n",
        "    ucb_policy = UCBPolicy(env.n_arms, c=2.0)\n",
        "    ucb_results = run_simulation(env, ucb_policy, n_rounds)\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Total Profit: ${ucb_results['total_reward']:.2f}\")\n",
        "    print(f\"  Average Profit per Round: ${ucb_results['average_reward']:.2f}\")\n",
        "    print(f\"\\n  Product Selection Counts:\")\n",
        "    for i, count in enumerate(ucb_results['action_counts']):\n",
        "        pct = (count / n_rounds) * 100\n",
        "        print(f\"    Product {i+1}: {count} times ({pct:.1f}%)\")\n",
        "\n",
        "    min_trials = np.argmin(ucb_results['action_counts'])\n",
        "    print(f\"\\n  Product with Minimal Trials: Product {min_trials + 1} ({ucb_results['action_counts'][min_trials]} times)\")\n",
        "    print(\"  This product was tried initially but received minimal trials later,\")\n",
        "    print(\"  indicating UCB quickly learned it was suboptimal.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Q6: COMPREHENSIVE COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    all_results = {\n",
        "        'Random': random_results,\n",
        "        'Greedy': greedy_results,\n",
        "        '\u03b5-Greedy (2%)': epsilon_results['\u03b5=2%'],\n",
        "        '\u03b5-Greedy (10%)': epsilon_results['\u03b5=10%'],\n",
        "        '\u03b5-Greedy (25%)': epsilon_results['\u03b5=25%'],\n",
        "        'UCB': ucb_results\n",
        "    }\n",
        "\n",
        "    print(\"\\nStrategy Performance Summary:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Strategy':<20} {'Total Profit':>15} {'Avg Profit/Round':>20} {'Best Product':>15}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for name, results in all_results.items():\n",
        "        best_product = np.argmax(results['action_counts']) + 1\n",
        "        print(f\"{name:<20} ${results['total_reward']:>14,.2f} ${results['average_reward']:>18,.2f} {'Product ' + str(best_product):>15}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_strategy = max(all_results.items(), key=lambda x: x[1]['total_reward'])\n",
        "    print(f\"\\nBest Strategy: {best_strategy[0]}\")\n",
        "    print(f\"Total Profit: ${best_strategy[1]['total_reward']:.2f}\")\n",
        "    print(f\"Average Profit per Round: ${best_strategy[1]['average_reward']:.2f}\")\n",
        "\n",
        "    most_profitable_product = np.argmax(analysis['mean_rewards']) + 1\n",
        "    print(f\"\\nMost Consistently Profitable Product: Product {most_profitable_product}\")\n",
        "    print(f\"Average Net Reward: ${analysis['mean_rewards'][most_profitable_product-1]:.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PLOTTING CUMULATIVE PROFIT CURVES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    plot_cumulative_rewards(all_results, \"Cumulative Net Profit: All Strategies\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CONCLUSIONS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\"\"\n",
        "1. The Random policy provides unstable and suboptimal performance, serving as a baseline.\n",
        "\n",
        "2. The Greedy policy can get stuck with suboptimal products due to premature lock-in.\n",
        "\n",
        "3. Epsilon-Greedy policies balance exploration and exploitation:\n",
        "   - Low exploration (2%): May miss better options\n",
        "   - Moderate exploration (10%): Good balance for this dataset\n",
        "   - High exploration (25%): Too much exploration reduces profit\n",
        "\n",
        "4. UCB intelligently explores uncertain products, often achieving the best performance.\n",
        "\n",
        "5. Learning-based approaches significantly outperform the random policy.\n",
        "\n",
        "RECOMMENDATION: Implement UCB or \u03b5-Greedy (10%) for production deployment.\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}