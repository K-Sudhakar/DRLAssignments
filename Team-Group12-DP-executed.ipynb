{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd14fc5",
   "metadata": {
    "id": "5cd14fc5"
   },
   "source": [
    "## Mini Chess Solver Using Dynamic Programming - Total 7 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49524177",
   "metadata": {
    "id": "49524177"
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "Design and implement a reinforcement learning agent using dynamic programming (value iteration or policy iteration) to compute an optimal policy for a simplified chess game. The agent plays as White and must learn how to convert an advantage into a win or at least avoid a loss in a MiniChess game against a defensive opponent. The problem must be modelled as a finite MDP. Register number of first student in a group (alphabetically sorted) will be considered for configuration design.\n",
    "The student will:\n",
    "* Implement a custom Mini Chess environment.\n",
    "* Use dynamic programming to compute the optimal value function and policy.\n",
    "* Analyze how state design and reward shaping affect the learned policy and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8d3b7",
   "metadata": {
    "id": "f2a8d3b7"
   },
   "source": [
    "### Scenario\n",
    "\n",
    "You are building a “Mini Chess Game” for beginner players. The coach focuses on a small, tractable game:\n",
    "* White: King + Pawn\n",
    "* Black: King\n",
    "* Board: 4×4 or 5×5 MiniChess board\n",
    "* White moves first and tries to either:\n",
    "    * Promote the pawn and then deliver checkmate, or\n",
    "    * Force a checkmate directly (if possible)\n",
    "\n",
    "Black tries to prevent this by blocking the pawn, chasing the white king, or capturing the pawn. The game is restricted to this small set of pieces and a tiny board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf39bfe",
   "metadata": {
    "id": "fbf39bfe"
   },
   "source": [
    "### Environment Description\n",
    "#### Board and Pieces\n",
    "* Board size:\n",
    "    * If the student roll number / registration number is even: use a 4×4 board (rows 0–3, cols 0–3).\n",
    "    * If odd: use a 5×5 board (rows 0–4, cols 0–4).\n",
    "* Pieces always present:\n",
    "    * White King (WK)\n",
    "    * White Pawn (WP)\n",
    "    * Black King (BK)\n",
    "    * No castling, no en passant, no promotion to anything other than Queen.\n",
    "* Legal Moves\n",
    "    * Kings move like normal chess kings - one square in any direction (8- neighborhood), staying on the board.\n",
    "    * Pawn:\n",
    "        * Moves one square forward (towards larger row index or smaller row index – the student must choose and clearly document a convention).\n",
    "        * Captures diagonally forward by one square.\n",
    "    * All usual constraints apply:\n",
    "        * Kings cannot move into check.\n",
    "        * Two kings may never occupy adjacent squares (illegal state).\n",
    "        * A piece cannot move through other pieces.\n",
    "* Episode Termination\n",
    "    * An episode ends when any of the following happens:\n",
    "        * Checkmate (White checkmates Black).\n",
    "        * Stalemate (side to move has no legal moves but is not in check).\n",
    "        * Pawn Capture (Black captures the White pawn).\n",
    "        * Pawn Promotion (White pawn reaches last rank and becomes a Queen). After promotion, they may either:\n",
    "                * (a) terminate immediately with a reward, or\n",
    "                * (b) continue playing with a Queen replacing the pawn.\n",
    "        * The student must choose one approach and justify it.\n",
    "        * Move limit exceeded (e.g., 20 or 30 plies) – draw\n",
    "1. State Space\n",
    "\n",
    "* Each state should minimally encode:\n",
    "    * Coordinates of WK: (r_wk, c_wk)\n",
    "    * Coordinates of WP (or a special value if promoted/captured): (r_wp, c_wp) or status flag\n",
    "    * Coordinates of BK: (r_bk, c_bk)\n",
    "    * Player to move: {White, Black}\n",
    "    * Any additional flags that can be necessary like,\n",
    "        * Has the pawn promoted?\n",
    "        * Check / checkmate / stalemate indicators.\n",
    "* The student must:\n",
    "    * Describe the state representation clearly.\n",
    "2. Action Space\n",
    "    * For each state, actions are the legal moves for the side to move:\n",
    "        * Move King to a legal square\n",
    "        * Move Pawn / promoted Queen\n",
    "    * The student must implement a function that, given a state, returns all legal actions.\n",
    "3. Rewards\n",
    "* The student has to define the reward schemes like:\n",
    "    * White checkmates Black: +10\n",
    "    * Pawn gets captured: -10\n",
    "    * Stalemate or draw by move limit: 0\n",
    "    * All non-terminal moves: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10018a",
   "metadata": {
    "id": "6c10018a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddea2384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:28.596380Z",
     "iopub.status.busy": "2025-12-25T15:37:28.596380Z",
     "iopub.status.idle": "2025-12-25T15:37:29.525568Z",
     "shell.execute_reply": "2025-12-25T15:37:29.525568Z"
    },
    "id": "ddea2384"
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional, Iterable, Any\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEFAULT_STUDENT_ID = \"2024AD05357\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab2df65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:29.529268Z",
     "iopub.status.busy": "2025-12-25T15:37:29.528132Z",
     "iopub.status.idle": "2025-12-25T15:37:29.538097Z",
     "shell.execute_reply": "2025-12-25T15:37:29.537085Z"
    },
    "id": "fab2df65"
   },
   "outputs": [],
   "source": [
    "# Basic types and helpers\n",
    "\n",
    "Pos = Tuple[int, int] # (row, col)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class State:\n",
    "    # define wk, wp, bk and the other things needed\n",
    "    wk: Pos\n",
    "    wp: Optional[Pos] # None if captured; when promoted keep the square and set `promoted=True`\n",
    "    bk: Pos\n",
    "    to_move: str # 'W' or 'B'\n",
    "    promoted: bool = False\n",
    "\n",
    "    def as_tuple(self):\n",
    "        return (self.wk, self.wp, self.bk, self.to_move, self.promoted)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Action:\n",
    "    piece: str # 'K' or 'P' or 'Q' (after promotion)\n",
    "    src: Pos\n",
    "    dst: Pos\n",
    "\n",
    "    def as_tuple(self):\n",
    "        return (self.piece, self.src, self.dst)\n",
    "\n",
    "def compute_unprotected_pawn_position(board_size: int) -> Pos:\n",
    "    center = board_size // 2\n",
    "    forward_row = min(board_size - 1, center + 1)\n",
    "    projected_col = center + 2\n",
    "    if projected_col >= board_size:\n",
    "        projected_col = max(0, center - 2)\n",
    "    return (forward_row, projected_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff83072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:29.541301Z",
     "iopub.status.busy": "2025-12-25T15:37:29.540075Z",
     "iopub.status.idle": "2025-12-25T15:37:29.570192Z",
     "shell.execute_reply": "2025-12-25T15:37:29.568660Z"
    },
    "id": "2ff83072"
   },
   "outputs": [],
   "source": [
    "# Define the MiniChess Environment - 1.5 mark\n",
    "\n",
    "class MiniChessEnv:\n",
    "    \"\"\"\n",
    "    MiniChess Environment:  King + Pawn vs King on a small board\n",
    "    \n",
    "    Design Choice: Pawn Promotion (Approach a)\n",
    "    - Upon promotion, the game ends immediately with reward +10\n",
    "    - No Queen play continuation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, board_size:  int = 4, max_plies: int = 30):\n",
    "        self.board_size = board_size\n",
    "        self.max_plies = max_plies\n",
    "        self.plies = 0\n",
    "        self.reset()\n",
    "\n",
    "    # ============================================\n",
    "    # Board and Position Helper Methods\n",
    "    # ============================================\n",
    "\n",
    "    def on_board(self, pos: Pos) -> bool:\n",
    "        \"\"\"Check if position is within board bounds\"\"\"\n",
    "        r, c = pos\n",
    "        return 0 <= r < self.board_size and 0 <= c < self.board_size\n",
    "\n",
    "    def king_moves(self, pos:  Pos) -> List[Pos]:\n",
    "        \"\"\"Generate all possible king moves from a position\"\"\"\n",
    "        r, c = pos\n",
    "        moves = []\n",
    "        for dr in [-1, 0, 1]:\n",
    "            for dc in [-1, 0, 1]:\n",
    "                if dr == 0 and dc == 0:\n",
    "                    continue\n",
    "                nxt = (r + dr, c + dc)\n",
    "                if self.on_board(nxt):\n",
    "                    moves.append(nxt)\n",
    "        return moves\n",
    "\n",
    "    def pawn_moves(self, state:  State) -> List[Pos]:\n",
    "        \"\"\"Generate all possible pawn moves (forward and diagonal captures)\"\"\"\n",
    "        moves = []\n",
    "        if state.wp is None:\n",
    "            return moves\n",
    "\n",
    "        r, c = state.wp\n",
    "        forward = (r + 1, c)\n",
    "        if self.on_board(forward):\n",
    "            moves.append(forward)\n",
    "\n",
    "        # Diagonal captures\n",
    "        for dc in [-1, 1]: \n",
    "            diag = (r + 1, c + dc)\n",
    "            if self.on_board(diag):\n",
    "                moves.append(diag)\n",
    "\n",
    "        return moves\n",
    "\n",
    "    # ============================================\n",
    "    # Game Rules and Legality Checks\n",
    "    # ============================================\n",
    "\n",
    "    def kings_adjacent(self, wk:  Pos, bk: Pos) -> bool:\n",
    "        \"\"\"Check if two kings are adjacent (illegal in chess)\"\"\"\n",
    "        return max(abs(wk[0] - bk[0]), abs(wk[1] - bk[1])) == 1\n",
    "\n",
    "    def in_check(self, king_pos: Pos, enemy_king: Pos) -> bool:\n",
    "        \"\"\"King is in check if adjacent to enemy king\"\"\"\n",
    "        return self.kings_adjacent(king_pos, enemy_king)\n",
    "\n",
    "    def is_checkmate(self, state: State) -> bool:\n",
    "        \"\"\"\n",
    "        Detect checkmate: King in check AND no legal moves\n",
    "        \"\"\"\n",
    "        if state.to_move == 'W':\n",
    "            king_pos = state.wk\n",
    "            enemy_king = state.bk\n",
    "        else:\n",
    "            king_pos = state.bk\n",
    "            enemy_king = state.wk\n",
    "\n",
    "        in_check = self.kings_adjacent(king_pos, enemy_king)\n",
    "        has_legal_moves = len(self.legal_actions(state)) > 0\n",
    "\n",
    "        return in_check and not has_legal_moves\n",
    "\n",
    "    def is_stalemate(self, state: State) -> bool:\n",
    "        \"\"\"\n",
    "        Detect stalemate: NOT in check AND no legal moves\n",
    "        \"\"\"\n",
    "        if state.to_move == 'W':\n",
    "            king_pos = state.wk\n",
    "            enemy_king = state. bk\n",
    "        else:\n",
    "            king_pos = state.bk\n",
    "            enemy_king = state. wk\n",
    "\n",
    "        in_check = self.kings_adjacent(king_pos, enemy_king)\n",
    "        has_legal_moves = len(self.legal_actions(state)) > 0\n",
    "\n",
    "        return not in_check and not has_legal_moves\n",
    "\n",
    "    def legal_actions(self, state: State) -> List[Action]:\n",
    "        \"\"\"Generate all legal actions for the given state\"\"\"\n",
    "        actions = []\n",
    "\n",
    "        if state.promoted:\n",
    "            # Game ends at promotion, no further moves\n",
    "            return actions\n",
    "\n",
    "        wk, wp, bk = state.wk, state.wp, state.bk\n",
    "\n",
    "        if state.to_move == 'W':\n",
    "            # White King moves\n",
    "            for dst in self.king_moves(wk):\n",
    "                if dst == wp or dst == bk:\n",
    "                    continue\n",
    "                if self.kings_adjacent(dst, bk):\n",
    "                    continue\n",
    "                actions.append(Action('K', wk, dst))\n",
    "\n",
    "            # White Pawn moves (only if pawn exists)\n",
    "            if wp is not None:\n",
    "                r, c = wp\n",
    "                forward = (r + 1, c)\n",
    "                if self.on_board(forward) and forward != wk and forward != bk:\n",
    "                    actions.append(Action('P', wp, forward))\n",
    "\n",
    "                # Diagonal captures (can only capture BK)\n",
    "                for dc in [-1, 1]: \n",
    "                    diag = (r + 1, c + dc)\n",
    "                    if diag == bk: \n",
    "                        actions.append(Action('P', wp, diag))\n",
    "\n",
    "        else:  # Black to move\n",
    "            for dst in self.king_moves(bk):\n",
    "                if dst == wk or dst == wp:\n",
    "                    continue\n",
    "                if self.kings_adjacent(dst, wk):\n",
    "                    continue\n",
    "                actions.append(Action('K', bk, dst))\n",
    "\n",
    "        return actions\n",
    "\n",
    "    # ============================================\n",
    "    # Transition / Step Function\n",
    "    # ============================================\n",
    "\n",
    "    def step(self, state: State, action: Action):\n",
    "        \"\"\"\n",
    "        Execute one action and return (next_state, reward, done, info)\n",
    "        This method mutates the environment's `plies` counter and is intended\n",
    "        for actual episode execution (not for planning/enumeration).\n",
    "        \"\"\"\n",
    "        wk, wp, bk = state.wk, state.wp, state.bk\n",
    "        to_move = state.to_move\n",
    "        promoted = state.promoted\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        # Increment move counter\n",
    "        self.plies += 1\n",
    "\n",
    "        if to_move == 'W': \n",
    "            if action.piece == 'K': \n",
    "                wk = action.dst\n",
    "            elif action.piece == 'P':\n",
    "                # Check if pawn captures Black King\n",
    "                if action.dst == bk:\n",
    "                    reward = 10\n",
    "                    done = True\n",
    "                    info['result'] = 'pawn_captured_king'\n",
    "\n",
    "                wp = action.dst\n",
    "\n",
    "                # Check for promotion\n",
    "                if wp[0] == self.board_size - 1:\n",
    "                    promoted = True\n",
    "                    reward = 10\n",
    "                    done = True\n",
    "                    info['result'] = 'pawn_promoted'\n",
    "\n",
    "            next_to_move = 'B'\n",
    "\n",
    "        else:  # Black move\n",
    "            bk = action.dst\n",
    "\n",
    "            # Check if Black King captures White Pawn\n",
    "            if wp is not None and bk == wp: \n",
    "                wp = None\n",
    "                reward = -10\n",
    "                done = True\n",
    "                info['result'] = 'pawn_captured'\n",
    "\n",
    "            next_to_move = 'W'\n",
    "\n",
    "        next_state = State(\n",
    "            wk=wk,\n",
    "            wp=wp,\n",
    "            bk=bk,\n",
    "            to_move=next_to_move,\n",
    "            promoted=promoted\n",
    "        )\n",
    "\n",
    "        # Check move limit BEFORE terminal state checks\n",
    "        if self.plies >= self.max_plies:\n",
    "            done = True\n",
    "            reward = 0\n",
    "            info['result'] = 'move_limit_exceeded'\n",
    "            return next_state, reward, done, info\n",
    "\n",
    "        # Only check for checkmate/stalemate if game not already ended\n",
    "        if not done:\n",
    "            if self.is_checkmate(next_state):\n",
    "                done = True\n",
    "                if next_state.to_move == 'B':\n",
    "                    reward = 10  # White checkmates Black\n",
    "                    info['result'] = 'white_checkmate'\n",
    "                else: \n",
    "                    reward = -10  # Black checkmates White\n",
    "                    info['result'] = 'black_checkmate'\n",
    "            elif self.is_stalemate(next_state):\n",
    "                done = True\n",
    "                reward = 0\n",
    "                info['result'] = 'stalemate'\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def step_from_state(self, state: State, action: Action):\n",
    "        \"\"\"Simulate a transition from `state` with `action` without mutating\n",
    "        the environment (no `self.plies` increment). Useful for BFS and DP.\n",
    "        Returns (next_state, reward, done, info).\n",
    "        \"\"\"\n",
    "        wk, wp, bk = state.wk, state.wp, state.bk\n",
    "        to_move = state.to_move\n",
    "        promoted = state.promoted\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        if to_move == 'W': \n",
    "            if action.piece == 'K': \n",
    "                wk = action.dst\n",
    "            elif action.piece == 'P':\n",
    "                # Check if pawn captures Black King\n",
    "                if action.dst == bk:\n",
    "                    reward = 10\n",
    "                    done = True\n",
    "                    info['result'] = 'pawn_captured_king'\n",
    "\n",
    "                wp = action.dst\n",
    "\n",
    "                # Check for promotion\n",
    "                if wp[0] == self.board_size - 1:\n",
    "                    promoted = True\n",
    "                    reward = 10\n",
    "                    done = True\n",
    "                    info['result'] = 'pawn_promoted'\n",
    "\n",
    "            next_to_move = 'B'\n",
    "\n",
    "        else:  # Black move\n",
    "            bk = action.dst\n",
    "\n",
    "            # Check if Black King captures White Pawn\n",
    "            if wp is not None and bk == wp: \n",
    "                wp = None\n",
    "                reward = -10\n",
    "                done = True\n",
    "                info['result'] = 'pawn_captured'\n",
    "\n",
    "            next_to_move = 'W'\n",
    "\n",
    "        next_state = State(\n",
    "            wk=wk,\n",
    "            wp=wp,\n",
    "            bk=bk,\n",
    "            to_move=next_to_move,\n",
    "            promoted=promoted\n",
    "        )\n",
    "\n",
    "        # Note: do NOT check/modify self.plies here (non-mutating simulation)\n",
    "        # Check move limit is not applied here because this is used for planning\n",
    "        # where plies should not be advanced.\n",
    "\n",
    "        # Only check for checkmate/stalemate if game not already ended\n",
    "        if not done:\n",
    "            if self.is_checkmate(next_state):\n",
    "                done = True\n",
    "                if next_state.to_move == 'B':\n",
    "                    reward = 10  # White checkmates Black\n",
    "                    info['result'] = 'white_checkmate'\n",
    "                else: \n",
    "                    reward = -10  # Black checkmates White\n",
    "                    info['result'] = 'black_checkmate'\n",
    "            elif self.is_stalemate(next_state):\n",
    "                done = True\n",
    "                reward = 0\n",
    "                info['result'] = 'stalemate'\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    # ============================================\n",
    "    # Reset and Rendering\n",
    "    # ============================================\n",
    "\n",
    "    def reset(self) -> State:\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.plies = 0\n",
    "        self.state = State(\n",
    "            wk=(0, 0),\n",
    "            wp=(1, 0),\n",
    "            bk=(self.board_size - 1, self.board_size - 1),\n",
    "            to_move='W',\n",
    "            promoted=False\n",
    "        )\n",
    "        return self.state\n",
    "\n",
    "    def render(self, state: Optional[State] = None):\n",
    "        \"\"\"Render the board state\"\"\"\n",
    "        if state is None: \n",
    "            state = self.state\n",
    "\n",
    "        board = [['.' for _ in range(self. board_size)] for _ in range(self.board_size)]\n",
    "\n",
    "        r, c = state.wk\n",
    "        board[r][c] = 'WK'\n",
    "\n",
    "        if state.wp is not None:\n",
    "            r, c = state.wp\n",
    "            board[r][c] = 'WP'\n",
    "\n",
    "        r, c = state.bk\n",
    "        board[r][c] = 'BK'\n",
    "\n",
    "        print(\"Board:\")\n",
    "        for i in range(self.board_size - 1, -1, -1):  # Print from top to bottom\n",
    "            print(f\"Row {i}: {' '.join(board[i])}\")\n",
    "        print(\"     \" + \" \".join([f\"C{i}\" for i in range(self. board_size)]))\n",
    "        print(f\"To move: {state.to_move}, Promoted: {state.promoted}, Plies: {self.plies}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8921af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:29.573200Z",
     "iopub.status.busy": "2025-12-25T15:37:29.573200Z",
     "iopub.status.idle": "2025-12-25T15:37:29.580851Z",
     "shell.execute_reply": "2025-12-25T15:37:29.579788Z"
    },
    "id": "0b8921af"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# State encoding & indexing\n",
    "# -----------------------------\n",
    "\n",
    "class StateIndexer:\n",
    "    def __init__(self, states: List[State]):\n",
    "        \"\"\"\n",
    "        Takes a list of states and builds bidirectional mappings\n",
    "        between State and integer indices.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.state_to_idx: Dict[State, int] = {}\n",
    "        self.idx_to_state: Dict[int, State] = {}\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        for idx, state in enumerate(self.states):\n",
    "            self.state_to_idx[state] = idx\n",
    "            self.idx_to_state[idx] = state\n",
    "\n",
    "    def encode(self, state: State) -> int:\n",
    "        return self.state_to_idx[state]\n",
    "\n",
    "    def decode(self, idx: int) -> State:\n",
    "        return self.idx_to_state[idx]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# List all reachable states\n",
    "# -----------------------------\n",
    "\n",
    "def list_reachable(env: MiniChessEnv, initial_state: State) -> List[State]:\n",
    "    \"\"\"\n",
    "    Enumerate all reachable states from an initial state using BFS.\n",
    "    Uses non-mutating simulation `step_from_state` to avoid changing env.plies.\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    queue = deque()\n",
    "\n",
    "    visited.add(initial_state)\n",
    "    queue.append(initial_state)\n",
    "\n",
    "    while queue:\n",
    "        state = queue.popleft()\n",
    "\n",
    "        # Skip terminal states\n",
    "        actions = env.legal_actions(state)\n",
    "        for action in actions:\n",
    "            next_state, _, done, _ = env.step_from_state(state, action)\n",
    "\n",
    "            if next_state not in visited:\n",
    "                visited.add(next_state)\n",
    "                if not done:\n",
    "                    queue.append(next_state)\n",
    "\n",
    "    return list(visited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44321269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:29.583237Z",
     "iopub.status.busy": "2025-12-25T15:37:29.583237Z",
     "iopub.status.idle": "2025-12-25T15:37:29.608275Z",
     "shell.execute_reply": "2025-12-25T15:37:29.606262Z"
    },
    "id": "44321269"
   },
   "outputs": [],
   "source": [
    "# Value Iteration / Policy Iteration\n",
    "\n",
    "def value_iteration(\n",
    "    env: MiniChessEnv,\n",
    "    states: list[State],\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-3\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute optimal value function and policy using value iteration.\n",
    "    Uses non-mutating `step_from_state` for transitions so env is not mutated.\n",
    "    \"\"\"\n",
    "    indexer = StateIndexer(states)\n",
    "    V = {s: 0.0 for s in states}\n",
    "    policy: Dict[State, Action] = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        iterations += 1\n",
    "\n",
    "        for state in states:\n",
    "            actions = env.legal_actions(state)\n",
    "\n",
    "            if not actions:\n",
    "                continue  # terminal state\n",
    "\n",
    "            best_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action in actions:\n",
    "                next_state, reward, done, _ = env.step_from_state(state, action)\n",
    "                value = reward\n",
    "                if not done and next_state in V:\n",
    "                    value += gamma * V[next_state]\n",
    "\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "\n",
    "            delta = max(delta, abs(V[state] - best_value))\n",
    "            V[state] = best_value\n",
    "            policy[state] = best_action\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    stats = {\n",
    "        \"iterations\": iterations,\n",
    "        \"final_delta\": delta,\n",
    "        \"runtime_sec\": runtime\n",
    "    }\n",
    "\n",
    "    return V, policy, stats\n",
    "\n",
    "\n",
    "def value_iteration_synchronous(\n",
    "    env: MiniChessEnv,\n",
    "    states: list[State],\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-3,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Synchronous Value Iteration: maintains separate V_old and V_new.\n",
    "    All value updates in an iteration use V_old; then V_old := V_new.\n",
    "    Useful for observing multi-iteration convergence.\n",
    "    \n",
    "    Returns (V, policy, stats) where stats includes per-iteration info.\n",
    "    \"\"\"\n",
    "    V_old = {s: 0.0 for s in states}\n",
    "    V_new = {s: 0.0 for s in states}\n",
    "    policy: Dict[State, Action] = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "    iteration_history = []\n",
    "    \n",
    "    while True:\n",
    "        iterations += 1\n",
    "        V_new = {s: 0.0 for s in states}\n",
    "        delta = 0.0\n",
    "        changed_count = 0\n",
    "        \n",
    "        for state in states:\n",
    "            actions = env.legal_actions(state)\n",
    "            \n",
    "            if not actions:\n",
    "                # Terminal state: no update\n",
    "                V_new[state] = V_old[state]\n",
    "                continue\n",
    "            \n",
    "            best_value = float('-inf')\n",
    "            best_action = None\n",
    "            \n",
    "            # Compute best action using V_old\n",
    "            for action in actions:\n",
    "                next_state, reward, done, _ = env.step_from_state(state, action)\n",
    "                value = reward\n",
    "                if not done and next_state in V_old:\n",
    "                    value += gamma * V_old[next_state]\n",
    "                \n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            \n",
    "            V_new[state] = best_value\n",
    "            policy[state] = best_action\n",
    "            \n",
    "            # Track changes\n",
    "            state_delta = abs(V_old[state] - V_new[state])\n",
    "            delta = max(delta, state_delta)\n",
    "            if state_delta > 1e-12:\n",
    "                changed_count += 1\n",
    "        \n",
    "        # Log iteration info\n",
    "        if verbose:\n",
    "            print(f\"Iteration {iterations}: delta={delta:.8f}, changed={changed_count}/{len(states)}\")\n",
    "        \n",
    "        iteration_history.append({\n",
    "            'iteration': iterations,\n",
    "            'delta': delta,\n",
    "            'changed': changed_count,\n",
    "            'n_states': len(states)\n",
    "        })\n",
    "        \n",
    "        # Swap: V_old := V_new for next iteration\n",
    "        V_old = V_new.copy()\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    stats = {\n",
    "        \"iterations\": iterations,\n",
    "        \"final_delta\": delta,\n",
    "        \"runtime_sec\": runtime,\n",
    "        \"iteration_history\": iteration_history\n",
    "    }\n",
    "    \n",
    "    return V_new, policy, stats\n",
    "\n",
    "\n",
    "# or\n",
    "\n",
    "def policy_iteration(\n",
    "    env: MiniChessEnv,\n",
    "    states: list[State],\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-3\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute optimal value function and policy using policy iteration.\n",
    "    Uses `step_from_state` to avoid mutating environment during planning.\n",
    "    \"\"\"\n",
    "    V = {s: 0.0 for s in states}\n",
    "    policy: Dict[State, Action] = {}\n",
    "\n",
    "    # Initialize random policy\n",
    "    for state in states:\n",
    "        actions = env.legal_actions(state)\n",
    "        if actions:\n",
    "            policy[state] = random.choice(actions)\n",
    "\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        iterations += 1\n",
    "\n",
    "        # -------------------------\n",
    "        # Policy Evaluation\n",
    "        # -------------------------\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "            for state in states:\n",
    "                action = policy.get(state)\n",
    "                if action is None:\n",
    "                    continue\n",
    "\n",
    "                next_state, reward, done, _ = env.step_from_state(state, action)\n",
    "                new_value = reward\n",
    "                if not done and next_state in V:\n",
    "                    new_value += gamma * V[next_state]\n",
    "\n",
    "                delta = max(delta, abs(V[state] - new_value))\n",
    "                V[state] = new_value\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        # -------------------------\n",
    "        # Policy Improvement\n",
    "        # -------------------------\n",
    "        policy_stable = True\n",
    "\n",
    "        for state in states:\n",
    "            actions = env.legal_actions(state)\n",
    "            if not actions:\n",
    "                continue\n",
    "\n",
    "            old_action = policy[state]\n",
    "            best_action = old_action\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            for action in actions:\n",
    "                next_state, reward, done, _ = env.step_from_state(state, action)\n",
    "                value = reward\n",
    "                if not done and next_state in V:\n",
    "                    value += gamma * V[next_state]\n",
    "\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "\n",
    "            policy[state] = best_action\n",
    "            if best_action != old_action:\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    stats = {\n",
    "        \"iterations\": iterations,\n",
    "        \"final_delta\": delta,\n",
    "        \"runtime_sec\": runtime\n",
    "    }\n",
    "\n",
    "    return V, policy, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d4bca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:29.612809Z",
     "iopub.status.busy": "2025-12-25T15:37:29.611800Z",
     "iopub.status.idle": "2025-12-25T15:37:29.623163Z",
     "shell.execute_reply": "2025-12-25T15:37:29.622153Z"
    },
    "id": "15d4bca3"
   },
   "outputs": [],
   "source": [
    "# Visualization - 0.5 mark\n",
    "\n",
    "def plot_value(\n",
    "    V: dict,\n",
    "    board_size: int,\n",
    "    fixed_wp: Pos,\n",
    "    fixed_bk: Pos,\n",
    "    to_move: str = 'W'\n",
    "):\n",
    "    \"\"\"\n",
    "    Fix pawn and black king positions.\n",
    "    Vary white king position and plot heatmap of V(s).\n",
    "    \"\"\"\n",
    "\n",
    "    heatmap = np.full((board_size, board_size), np.nan)\n",
    "\n",
    "    for r in range(board_size):\n",
    "        for c in range(board_size):\n",
    "            wk = (r, c)\n",
    "\n",
    "            # Skip illegal overlaps\n",
    "            if wk == fixed_wp or wk == fixed_bk:\n",
    "                continue\n",
    "\n",
    "            # Kings cannot be adjacent\n",
    "            if max(abs(wk[0] - fixed_bk[0]), abs(wk[1] - fixed_bk[1])) == 1:\n",
    "                continue\n",
    "\n",
    "            state = State(\n",
    "                wk=wk,\n",
    "                wp=fixed_wp,\n",
    "                bk=fixed_bk,\n",
    "                to_move=to_move,\n",
    "                promoted=False\n",
    "            )\n",
    "\n",
    "            if state in V:\n",
    "                heatmap[r, c] = V[state]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(heatmap, origin=\"lower\")\n",
    "    plt.colorbar(label=\"V(s)\")\n",
    "    plt.title(\"State-Value Function Heatmap (White King Position)\")\n",
    "    plt.xlabel(\"Column\")\n",
    "    plt.ylabel(\"Row\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66125bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:29.627630Z",
     "iopub.status.busy": "2025-12-25T15:37:29.626632Z",
     "iopub.status.idle": "2025-12-25T15:37:30.036755Z",
     "shell.execute_reply": "2025-12-25T15:37:30.035338Z"
    },
    "id": "66125bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student ID: 2024AD05357\n",
      "Student ID Last Digit: 7\n",
      "Board Size:  5x5\n",
      "Max Plies: 30\n",
      "============================================================\n",
      "Initial Configuration: Config 2: Center vs Border\n",
      "\n",
      "Initial State:\n",
      "Board:\n",
      "Row 4: . . . . .\n",
      "Row 3: . . . . WP\n",
      "Row 2: . . WK . .\n",
      "Row 1: . . . . .\n",
      "Row 0: . . . . BK\n",
      "     C0 C1 C2 C3 C4\n",
      "To move: W, Promoted: False, Plies: 0\n",
      "\n",
      "Enumerating reachable states using BFS...\n",
      "Total reachable states: 1227\n",
      "\n",
      "Running Value Iteration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Iteration Statistics:\n",
      "  Iterations: 3\n",
      "  Final Delta: 0.000000\n",
      "  Runtime: 0.2802 seconds\n",
      "  Total Time (including enumeration): 0.2812 seconds\n",
      "\n",
      "Initial State Value: 10.0000\n",
      "Optimal Action from Initial State: Action(piece='P', src=(3, 4), dst=(4, 4))\n",
      "\n",
      "Generating state-value function heatmap...\n",
      "\n",
      "Sample of learned policy (first 10 states with actions):\n",
      "----------------------------------------------------------------------\n",
      "State:  WK=(4, 0), WP=(3, 4), BK=(2, 1), Turn=B\n",
      "  Value: 9.9000\n",
      "  Action:  K (2, 1) -> (1, 0)\n",
      "count in main:  0\n",
      "State:  WK=(4, 3), WP=(3, 4), BK=(3, 0), Turn=B\n",
      "  Value: 9.9000\n",
      "  Action:  K (3, 0) -> (2, 0)\n",
      "count in main:  1\n",
      "State:  WK=(4, 4), WP=(3, 4), BK=(1, 0), Turn=W\n",
      "  Value: 9.8010\n",
      "  Action:  K (4, 4) -> (3, 3)\n",
      "count in main:  2\n",
      "State:  WK=(0, 3), WP=(3, 4), BK=(2, 3), Turn=B\n",
      "  Value: 9.9000\n",
      "  Action:  K (2, 3) -> (2, 2)\n",
      "count in main:  3\n",
      "State:  WK=(3, 1), WP=(3, 4), BK=(1, 1), Turn=W\n",
      "  Value: 10.0000\n",
      "  Action:  P (3, 4) -> (4, 4)\n",
      "count in main:  4\n",
      "State:  WK=(2, 3), WP=(3, 4), BK=(4, 4), Turn=W\n",
      "  Value: 9.8010\n",
      "  Action:  K (2, 3) -> (1, 2)\n",
      "count in main:  5\n",
      "State:  WK=(0, 4), WP=(3, 4), BK=(2, 1), Turn=W\n",
      "  Value: 10.0000\n",
      "  Action:  P (3, 4) -> (4, 4)\n",
      "count in main:  6\n",
      "State:  WK=(3, 1), WP=(3, 4), BK=(0, 3), Turn=B\n",
      "  Value: 9.9000\n",
      "  Action:  K (0, 3) -> (0, 2)\n",
      "count in main:  7\n",
      "State:  WK=(0, 4), WP=(3, 4), BK=(2, 3), Turn=W\n",
      "  Value: 10.0000\n",
      "  Action:  P (3, 4) -> (4, 4)\n",
      "count in main:  8\n",
      "State:  WK=(1, 1), WP=(3, 4), BK=(4, 4), Turn=W\n",
      "  Value: 9.8010\n",
      "  Action:  K (1, 1) -> (0, 0)\n",
      "count in main:  9\n",
      "\n",
      "======================================================================\n",
      "Running a sample episode using learned policy:\n",
      "======================================================================\n",
      "\n",
      "Step 1:\n",
      "Board:\n",
      "Row 4: . . . . .\n",
      "Row 3: . . . . WP\n",
      "Row 2: . . WK . .\n",
      "Row 1: . . . . .\n",
      "Row 0: . . . . BK\n",
      "     C0 C1 C2 C3 C4\n",
      "To move: W, Promoted: False, Plies: 0\n",
      "\n",
      "Action: P moves from (3, 4) to (4, 4)\n",
      "Reward: 10\n",
      "Result: pawn_promoted\n",
      "\n",
      "Episode ended after 1 steps\n",
      "Total reward: 10\n",
      "Termination reason: pawn_promoted\n",
      "\n",
      "======================================================================\n",
      "Demo Complete!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SKatam\\AppData\\Local\\Temp\\ipykernel_43344\\2485074994.py:46: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Main Usage\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # =====================================================\n",
    "    # Environment setup based on Student ID\n",
    "    # =====================================================\n",
    "    \n",
    "    STUDENT_ID = DEFAULT_STUDENT_ID\n",
    "    STUDENT_ID_LAST_DIGIT = int(STUDENT_ID[-1])\n",
    "    \n",
    "    # Board size depends on ID parity\n",
    "    if STUDENT_ID_LAST_DIGIT % 2 == 0:\n",
    "        BOARD_SIZE = 4  # Even:  4x4 board\n",
    "    else:\n",
    "        BOARD_SIZE = 5  # Odd:  5x5 board\n",
    "    \n",
    "    MAX_PLIES = 30\n",
    "    \n",
    "    print(f\"Student ID: {STUDENT_ID}\")\n",
    "    print(f\"Student ID Last Digit: {STUDENT_ID_LAST_DIGIT}\")\n",
    "    print(f\"Board Size:  {BOARD_SIZE}x{BOARD_SIZE}\")\n",
    "    print(f\"Max Plies: {MAX_PLIES}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    env = MiniChessEnv(board_size=BOARD_SIZE, max_plies=MAX_PLIES)\n",
    "\n",
    "    # =====================================================\n",
    "    # Initial state configuration (ID-based)\n",
    "    # =====================================================\n",
    "    if STUDENT_ID_LAST_DIGIT <= 4:\n",
    "        # Configuration 1: WK and BK on opposite corners, pawn near White\n",
    "        initial_state = State(\n",
    "            wk=(0, 0),\n",
    "            wp=(1, 0),\n",
    "            bk=(BOARD_SIZE - 1, BOARD_SIZE - 1),\n",
    "            to_move='W',\n",
    "            promoted=False\n",
    "        )\n",
    "        config_name = \"Config 1: Opposite Corners\"\n",
    "    elif STUDENT_ID_LAST_DIGIT <= 9:\n",
    "        # Configuration 2: WK near center, BK on border, pawn in front of WK\n",
    "        center = BOARD_SIZE // 2\n",
    "        pawn_pos = compute_unprotected_pawn_position(BOARD_SIZE)\n",
    "        initial_state = State(\n",
    "            wk=(center, center),\n",
    "            wp=pawn_pos,\n",
    "            bk=(0, BOARD_SIZE - 1),\n",
    "            to_move='W',\n",
    "            promoted=False\n",
    "        )\n",
    "        config_name = \"Config 2: Center vs Border\"\n",
    "    else:\n",
    "        # Configuration 3: Alternative setup\n",
    "        initial_state = State(\n",
    "            wk=(1, 1),\n",
    "            wp=(2, 1),\n",
    "            bk=(BOARD_SIZE - 2, BOARD_SIZE - 2),\n",
    "            to_move='W',\n",
    "            promoted=False\n",
    "        )\n",
    "        config_name = \"Config 3: Symmetrical\"\n",
    "\n",
    "    print(f\"Initial Configuration: {config_name}\\n\")\n",
    "    print(\"Initial State:\")\n",
    "    env.render(initial_state)\n",
    "\n",
    "    # =====================================================\n",
    "    # Enumerate reachable states using BFS\n",
    "    # =====================================================\n",
    "    print(\"Enumerating reachable states using BFS...\")\n",
    "    reachable_states = list_reachable(env, initial_state)\n",
    "    print(f\"Total reachable states: {len(reachable_states)}\\n\")\n",
    "\n",
    "    # =====================================================\n",
    "    # Run Dynamic Programming (Value Iteration)\n",
    "    # =====================================================\n",
    "    print(\"Running Value Iteration...\")\n",
    "    start_time = time.time()\n",
    "    V, policy, stats = value_iteration(env, reachable_states, gamma=0.99, theta=1e-4)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(\"\\nValue Iteration Statistics:\")\n",
    "    print(f\"  Iterations: {stats['iterations']}\")\n",
    "    print(f\"  Final Delta: {stats['final_delta']:.6f}\")\n",
    "    print(f\"  Runtime: {stats['runtime_sec']:.4f} seconds\")\n",
    "    print(f\"  Total Time (including enumeration): {elapsed:.4f} seconds\\n\")\n",
    "\n",
    "    # =====================================================\n",
    "    # Analyze initial state value\n",
    "    # =====================================================\n",
    "    if initial_state in V:\n",
    "        initial_value = V[initial_state]\n",
    "        print(f\"Initial State Value: {initial_value:.4f}\")\n",
    "        if initial_state in policy:\n",
    "            action = policy[initial_state]\n",
    "            print(f\"Optimal Action from Initial State: {action}\\n\")\n",
    "    else:\n",
    "        print(\"Initial state not in value function (terminal or unreachable)\\n\")\n",
    "\n",
    "    # =====================================================\n",
    "    # Visualize value function (heatmap)\n",
    "    # =====================================================\n",
    "    print(\"Generating state-value function heatmap...\")\n",
    "    if initial_state. wp is not None and initial_state.bk is not None:\n",
    "        plot_value(\n",
    "            V=V,\n",
    "            board_size=BOARD_SIZE,\n",
    "            fixed_wp=initial_state.wp,\n",
    "            fixed_bk=initial_state.bk,\n",
    "            to_move='W'\n",
    "        )\n",
    "\n",
    "    # =====================================================\n",
    "    # Show learned policy for sample states\n",
    "    # =====================================================\n",
    "    print(\"\\nSample of learned policy (first 10 states with actions):\")\n",
    "    print(\"-\" * 70)\n",
    "    count = 0\n",
    "    for state in reachable_states: \n",
    "        if state in policy and count < 10:\n",
    "            action = policy[state]\n",
    "            value = V. get(state, 0.0)\n",
    "            print(f\"State:  WK={state.wk}, WP={state.wp}, BK={state.bk}, Turn={state.to_move}\")\n",
    "            print(f\"  Value: {value:.4f}\")\n",
    "            print(f\"  Action:  {action. piece} {action.src} -> {action.dst}\")\n",
    "            print(\"count in main: \", count)\n",
    "            count += 1\n",
    "\n",
    "    # =====================================================\n",
    "    # Run a sample episode using the learned policy\n",
    "    # =====================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Running a sample episode using learned policy:\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    env_play = MiniChessEnv(board_size=BOARD_SIZE, max_plies=MAX_PLIES)\n",
    "    current_state = initial_state\n",
    "    step_count = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while step_count < 20:  # Safety limit\n",
    "        print(f\"Step {step_count + 1}:\")\n",
    "        env_play.render(current_state)\n",
    "        \n",
    "        actions = env_play.legal_actions(current_state)\n",
    "        \n",
    "        if not actions:\n",
    "            print(\"No legal actions available.  Episode ends.\")\n",
    "            break\n",
    "        \n",
    "        # Get optimal action\n",
    "        if current_state in policy:\n",
    "            action = policy[current_state]\n",
    "        else: \n",
    "            # Fallback: random action\n",
    "            action = random.choice(actions)\n",
    "            print(\"(Using random action as state not in policy)\")\n",
    "        \n",
    "        next_state, reward, done, info = env_play.step(current_state, action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        print(f\"Action: {action.piece} moves from {action.src} to {action.dst}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        if info:\n",
    "            print(f\"Result: {info. get('result', 'ongoing')}\")\n",
    "        \n",
    "        current_state = next_state\n",
    "        step_count += 1\n",
    "        \n",
    "        if done:\n",
    "            print(f\"\\nEpisode ended after {step_count} steps\")\n",
    "            print(f\"Total reward: {episode_reward}\")\n",
    "            print(f\"Termination reason: {info.get('result', 'unknown')}\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Demo Complete!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4de05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:30.039208Z",
     "iopub.status.busy": "2025-12-25T15:37:30.039208Z",
     "iopub.status.idle": "2025-12-25T15:37:30.044704Z",
     "shell.execute_reply": "2025-12-25T15:37:30.043658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Result Discussion and descriptive answers, conclusion - 1 mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca41de8",
   "metadata": {},
   "source": [
    "# Design Choices and Justifications\n",
    "\n",
    "## State Representation\n",
    "- **Pawn Promotion Handling:** We implement **Approach (a)** - immediate termination at promotion\n",
    "  - **Rationale:** Simplifies the state space by avoiding Queen movement complexity\n",
    "  - **Alternative:** Approach (b) would require implementing full Queen movement and new checkmate detection logic, which significantly expands the state space\n",
    "  - **Impact:** The agent learns to prioritize pawn promotion as a winning strategy\n",
    "\n",
    "## Checkmate vs.  Stalemate\n",
    "- **Checkmate:** King is in check AND has no legal moves → **White wins (+10)**\n",
    "- **Stalemate:** King is NOT in check but has no legal moves → **Draw (0)**\n",
    "- **Implementation:** Separate methods `is_checkmate()` and `is_stalemate()` provide clear distinction\n",
    "\n",
    "## Move Limit (max_plies)\n",
    "- Episodes terminate after `max_plies` moves with reward 0 (draw)\n",
    "- Default:  30 moves (60 half-moves in standard chess notation)\n",
    "- Prevents infinite loops in learning\n",
    "\n",
    "## Reward Shaping\n",
    "- Immediate rewards:  +10 (White wins), -10 (White loses), 0 (draw/ongoing)\n",
    "- Sparse reward design encourages multi-step planning\n",
    "- No intermediate rewards encourage efficient solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293e5898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:37:30.046903Z",
     "iopub.status.busy": "2025-12-25T15:37:30.046903Z",
     "iopub.status.idle": "2025-12-25T15:37:31.308776Z",
     "shell.execute_reply": "2025-12-25T15:37:31.307759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "STUDENT ID: 2024AD05357\n",
      "==========================================================================================\n",
      "Last Digit: 7\n",
      "Board Size: 5x5\n",
      "Max Plies: 30\n",
      "Configuration: Config 2: Center vs Border\n",
      "\n",
      "Initial Board State:\n",
      "Board:\n",
      "Row 4: . . . . .\n",
      "Row 3: . . . . WP\n",
      "Row 2: . . WK . .\n",
      "Row 1: . . . . .\n",
      "Row 0: . . . . BK\n",
      "     C0 C1 C2 C3 C4\n",
      "To move: W, Promoted: False, Plies: 0\n",
      "\n",
      "Enumerating reachable states...\n",
      "Total Reachable States: 1227\n",
      "\n",
      "Running Synchronous Value Iteration (gamma=0.99, theta=1e-4)...\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: delta=10.00000000, changed=376/1227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: delta=9.90000000, changed=395/1227\n",
      "Iteration 3: delta=9.80100000, changed=38/1227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: delta=9.70299000, changed=19/1227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: delta=0.00000000, changed=0/1227\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Value Iteration Statistics:\n",
      "  Total Iterations: 5\n",
      "  Final Delta: 0.00000000\n",
      "  Runtime: 0.425871 seconds\n",
      "\n",
      "Initial State Value: 10.000000\n",
      "Optimal Action from Initial State: Action(piece='P', src=(3, 4), dst=(4, 4))\n",
      "\n",
      "Learned Policy (all 828 states):\n",
      "------------------------------------------------------------------------------------------\n",
      "State 21: WK=(1, 4) WP=(3, 4) BK=(3, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 31: WK=(2, 4) WP=(3, 4) BK=(0, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 0) → (0, 1)\n",
      "State 41: WK=(3, 0) WP=(3, 4) BK=(1, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (1, 1) → (0, 0)\n",
      "State 81: WK=(1, 1) WP=(3, 4) BK=(4, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 91: WK=(0, 1) WP=(3, 4) BK=(4, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 3) → (3, 2)\n",
      "State 101: WK=(1, 1) WP=(3, 4) BK=(2, 4) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 4) → (1, 3)\n",
      "State 111: WK=(1, 1) WP=(3, 4) BK=(4, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 161: WK=(2, 3) WP=(3, 4) BK=(2, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 171: WK=(3, 0) WP=(3, 4) BK=(3, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 181: WK=(4, 3) WP=(3, 4) BK=(2, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 191: WK=(1, 3) WP=(3, 4) BK=(4, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 221: WK=(1, 4) WP=(3, 4) BK=(3, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 231: WK=(0, 2) WP=(3, 4) BK=(0, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 0) → (1, 0)\n",
      "State 241: WK=(2, 1) WP=(3, 4) BK=(4, 2) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 2) → (3, 3)\n",
      "State 251: WK=(0, 1) WP=(3, 4) BK=(2, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 261: WK=(2, 4) WP=(3, 4) BK=(3, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 281: WK=(4, 4) WP=(3, 4) BK=(1, 3) Turn=W\n",
      "  Value: 9.801000\n",
      "  Action: K (4, 4) → (3, 3)\n",
      "State 291: WK=(4, 0) WP=(3, 4) BK=(0, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 0) → (0, 1)\n",
      "State 311: WK=(0, 4) WP=(3, 4) BK=(3, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 321: WK=(2, 3) WP=(3, 4) BK=(2, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 331: WK=(1, 2) WP=(3, 4) BK=(2, 4) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 4) → (1, 4)\n",
      "State 351: WK=(3, 3) WP=(3, 4) BK=(3, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (3, 1) → (2, 0)\n",
      "State 361: WK=(0, 3) WP=(3, 4) BK=(4, 3) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 371: WK=(4, 4) WP=(3, 4) BK=(2, 4) Turn=W\n",
      "  Value: 9.801000\n",
      "  Action: K (4, 4) → (4, 3)\n",
      "State 401: WK=(0, 2) WP=(3, 4) BK=(1, 4) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 411: WK=(2, 2) WP=(3, 4) BK=(0, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 421: WK=(1, 0) WP=(3, 4) BK=(3, 3) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 431: WK=(2, 0) WP=(3, 4) BK=(2, 2) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 2) → (1, 2)\n",
      "State 441: WK=(2, 3) WP=(3, 4) BK=(0, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 1) → (0, 0)\n",
      "State 451: WK=(2, 3) WP=(3, 4) BK=(0, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 461: WK=(2, 0) WP=(3, 4) BK=(2, 4) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 471: WK=(2, 0) WP=(3, 4) BK=(2, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 3) → (1, 2)\n",
      "State 491: WK=(0, 1) WP=(3, 4) BK=(2, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 0) → (2, 1)\n",
      "State 521: WK=(4, 0) WP=(3, 4) BK=(1, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (1, 3) → (0, 2)\n",
      "State 531: WK=(4, 4) WP=(3, 4) BK=(4, 1) Turn=W\n",
      "  Value: 9.801000\n",
      "  Action: K (4, 4) → (3, 3)\n",
      "State 541: WK=(2, 1) WP=(3, 4) BK=(4, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 0) → (4, 1)\n",
      "State 551: WK=(0, 4) WP=(3, 4) BK=(2, 4) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 4) → (2, 3)\n",
      "State 561: WK=(2, 2) WP=(3, 4) BK=(1, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 581: WK=(1, 1) WP=(3, 4) BK=(3, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 591: WK=(1, 3) WP=(3, 4) BK=(0, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 601: WK=(4, 0) WP=(3, 4) BK=(0, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 1) → (0, 0)\n",
      "State 611: WK=(2, 4) WP=(3, 4) BK=(0, 3) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 621: WK=(4, 4) WP=(3, 4) BK=(2, 3) Turn=W\n",
      "  Value: 9.801000\n",
      "  Action: K (4, 4) → (4, 3)\n",
      "State 631: WK=(0, 1) WP=(3, 4) BK=(2, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 681: WK=(2, 2) WP=(3, 4) BK=(0, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 1) → (0, 0)\n",
      "State 711: WK=(1, 0) WP=(3, 4) BK=(4, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 721: WK=(4, 0) WP=(3, 4) BK=(4, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 3) → (3, 2)\n",
      "State 751: WK=(4, 4) WP=(3, 4) BK=(0, 3) Turn=W\n",
      "  Value: 9.801000\n",
      "  Action: K (4, 4) → (3, 3)\n",
      "State 771: WK=(3, 0) WP=(3, 4) BK=(1, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 801: WK=(2, 2) WP=(3, 4) BK=(4, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 3) → (4, 2)\n",
      "State 811: WK=(3, 0) WP=(3, 4) BK=(2, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 821: WK=(0, 0) WP=(3, 4) BK=(2, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 831: WK=(2, 2) WP=(3, 4) BK=(1, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (1, 0) → (0, 0)\n",
      "State 841: WK=(3, 3) WP=(3, 4) BK=(2, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 861: WK=(0, 0) WP=(3, 4) BK=(3, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 871: WK=(3, 0) WP=(3, 4) BK=(0, 2) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 2) → (0, 1)\n",
      "State 881: WK=(2, 1) WP=(3, 4) BK=(2, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 3) → (1, 3)\n",
      "State 891: WK=(0, 1) WP=(3, 4) BK=(4, 2) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 2) → (3, 1)\n",
      "State 901: WK=(2, 1) WP=(3, 4) BK=(1, 3) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 921: WK=(2, 4) WP=(3, 4) BK=(1, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 931: WK=(4, 0) WP=(3, 4) BK=(2, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 941: WK=(0, 3) WP=(3, 4) BK=(2, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 0) → (1, 0)\n",
      "State 951: WK=(3, 2) WP=(3, 4) BK=(2, 4) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 4) → (1, 3)\n",
      "State 981: WK=(3, 0) WP=(3, 4) BK=(4, 4) Turn=W\n",
      "  Value: 9.801000\n",
      "  Action: K (3, 0) → (2, 0)\n",
      "State 991: WK=(2, 0) WP=(3, 4) BK=(1, 4) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (1, 4) → (0, 3)\n",
      "State 1001: WK=(2, 3) WP=(3, 4) BK=(3, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (3, 1) → (2, 0)\n",
      "State 1011: WK=(1, 4) WP=(3, 4) BK=(3, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (3, 3) → (2, 2)\n",
      "State 1031: WK=(2, 0) WP=(3, 4) BK=(4, 2) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 2) → (3, 2)\n",
      "State 1041: WK=(1, 3) WP=(3, 4) BK=(2, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 0) → (1, 0)\n",
      "State 1051: WK=(0, 2) WP=(3, 4) BK=(4, 4) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 4) → (3, 3)\n",
      "State 1061: WK=(2, 3) WP=(3, 4) BK=(4, 3) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (4, 3) → (4, 2)\n",
      "State 1071: WK=(0, 0) WP=(3, 4) BK=(2, 3) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 1081: WK=(3, 1) WP=(3, 4) BK=(0, 2) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (0, 2) → (0, 1)\n",
      "State 1091: WK=(1, 2) WP=(3, 4) BK=(2, 0) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 1111: WK=(0, 1) WP=(3, 4) BK=(0, 4) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 1161: WK=(4, 2) WP=(3, 4) BK=(2, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (2, 1) → (1, 0)\n",
      "State 1171: WK=(3, 3) WP=(3, 4) BK=(1, 1) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (1, 1) → (0, 0)\n",
      "State 1191: WK=(0, 3) WP=(3, 4) BK=(3, 0) Turn=B\n",
      "  Value: 9.900000\n",
      "  Action: K (3, 0) → (2, 0)\n",
      "State 1201: WK=(2, 4) WP=(3, 4) BK=(0, 2) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 1211: WK=(3, 2) WP=(3, 4) BK=(1, 1) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "State 1221: WK=(0, 1) WP=(3, 4) BK=(1, 4) Turn=W\n",
      "  Value: 10.000000\n",
      "  Action: P (3, 4) → (4, 4)\n",
      "\n",
      "==========================================================================================\n",
      "SUMMARY OF ALL STUDENTS\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Student ID  Last Digit  Board Size                     Config  Reachable States  VI Iterations VI Runtime (s) Initial Value                            Initial Action\n",
      "2024AD05357           7           5 Config 2: Center vs Border              1227              5       0.425871     10.000000 Action(piece='P', src=(3, 4), dst=(4, 4))\n"
     ]
    }
   ],
   "source": [
    "# Per-Student Analysis: Synchronous Value Iteration with Multi-Iteration Convergence\n",
    "# Note: We are taking 1st odd ID for eveluation.\n",
    "STUDENT_IDS = [\n",
    "    '2024AD05357'\n",
    "]\n",
    "\n",
    "def build_initial_state_for_id_digit(digit, board_size):\n",
    "    if digit <= 4:\n",
    "        return State(wk=(0,0), wp=(1,0), bk=(board_size-1, board_size-1), to_move='W', promoted=False), 'Config 1: Opposite Corners'\n",
    "    elif digit <= 9:\n",
    "        center = board_size // 2\n",
    "        pawn_pos = compute_unprotected_pawn_position(board_size)\n",
    "        return State(wk=(center, center), wp=pawn_pos, bk=(0, board_size-1), to_move='W', promoted=False), 'Config 2: Center vs Border'\n",
    "    else:\n",
    "        return State(wk=(1,1), wp=(2,1), bk=(board_size-2, board_size-2), to_move='W', promoted=False), 'Config 3: Symmetrical'\n",
    "\n",
    "all_student_results = []\n",
    "\n",
    "for student_id in STUDENT_IDS:\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"STUDENT ID: {student_id}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Extract last digit\n",
    "    try:\n",
    "        last_digit = int(student_id.strip()[-1])\n",
    "    except Exception:\n",
    "        last_digit = 0\n",
    "    \n",
    "    # Board size based on parity\n",
    "    board_size = 4 if last_digit % 2 == 0 else 5\n",
    "    max_plies = 30\n",
    "    \n",
    "    print(f\"Last Digit: {last_digit}\")\n",
    "    print(f\"Board Size: {board_size}x{board_size}\")\n",
    "    print(f\"Max Plies: {max_plies}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env_student = MiniChessEnv(board_size=board_size, max_plies=max_plies)\n",
    "    \n",
    "    # Get initial state\n",
    "    initial_state_s, config_name = build_initial_state_for_id_digit(last_digit, board_size)\n",
    "    print(f\"Configuration: {config_name}\\n\")\n",
    "    print(\"Initial Board State:\")\n",
    "    env_student.render(initial_state_s)\n",
    "    \n",
    "    # Enumerate reachable states\n",
    "    print(f\"Enumerating reachable states...\")\n",
    "    reachable_states_s = list_reachable(env_student, initial_state_s)\n",
    "    print(f\"Total Reachable States: {len(reachable_states_s)}\\n\")\n",
    "    \n",
    "    # Run Synchronous Value Iteration with verbose output\n",
    "    print(f\"Running Synchronous Value Iteration (gamma=0.99, theta=1e-4)...\")\n",
    "    print(\"-\" * 90)\n",
    "    V_s, policy_s, stats_s = value_iteration_synchronous(\n",
    "        env_student, reachable_states_s, gamma=0.99, theta=1e-4, verbose=True\n",
    "    )\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    print(f\"\\nValue Iteration Statistics:\")\n",
    "    print(f\"  Total Iterations: {stats_s['iterations']}\")\n",
    "    print(f\"  Final Delta: {stats_s['final_delta']:.8f}\")\n",
    "    print(f\"  Runtime: {stats_s['runtime_sec']:.6f} seconds\\n\")\n",
    "    \n",
    "    # Initial state value and optimal action\n",
    "    initial_val_s = V_s.get(initial_state_s, 0.0)\n",
    "    initial_action_s = policy_s.get(initial_state_s, None)\n",
    "    print(f\"Initial State Value: {initial_val_s:.6f}\")\n",
    "    print(f\"Optimal Action from Initial State: {initial_action_s}\\n\")\n",
    "    \n",
    "    # Print all learned policies\n",
    "    print(f\"Learned Policy (all {len(policy_s)} states):\")\n",
    "    print(\"-\" * 90)\n",
    "    for idx, state in enumerate(reachable_states_s):\n",
    "        if state in policy_s:\n",
    "            count +=1\n",
    "            action = policy_s[state]\n",
    "            value = V_s.get(state, 0.0)\n",
    "            if (idx % 10 == 0):\n",
    "                print(f\"State {idx+1}: WK={state.wk} WP={state.wp} BK={state.bk} Turn={state.to_move}\")\n",
    "                print(f\"  Value: {value:.6f}\")\n",
    "                print(f\"  Action: {action.piece} {action.src} → {action.dst}\")\n",
    "    \n",
    "    # Store results\n",
    "    all_student_results.append({\n",
    "        'student_id': student_id,\n",
    "        'last_digit': last_digit,\n",
    "        'board_size': board_size,\n",
    "        'config': config_name,\n",
    "        'n_states': len(reachable_states_s),\n",
    "        'vi_iterations': stats_s['iterations'],\n",
    "        'vi_runtime': stats_s['runtime_sec'],\n",
    "        'initial_value': initial_val_s,\n",
    "        'initial_action': initial_action_s,\n",
    "        'V': V_s,\n",
    "        'policy': policy_s,\n",
    "        'iteration_history': stats_s.get('iteration_history', [])\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SUMMARY OF ALL STUDENTS\")\n",
    "print(\"=\" * 90)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\n",
    "            'Student ID': r['student_id'],\n",
    "            'Last Digit': r['last_digit'],\n",
    "            'Board Size': r['board_size'],\n",
    "            'Config': r['config'],\n",
    "            'Reachable States': r['n_states'],\n",
    "            'VI Iterations': r['vi_iterations'],\n",
    "            'VI Runtime (s)': f\"{r['vi_runtime']:.6f}\",\n",
    "            'Initial Value': f\"{r['initial_value']:.6f}\",\n",
    "            'Initial Action': str(r['initial_action'])\n",
    "        }\n",
    "        for r in all_student_results\n",
    "    ])\n",
    "    print(summary_df.to_string(index=False))\n",
    "    #summary_df.to_csv('student_dp_analysis_synchronous.csv', index=False)\n",
    "    #print(f\"\\nResults saved to: student_dp_analysis_synchronous.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating summary: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
