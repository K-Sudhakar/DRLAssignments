{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Part 2: Dynamic Programming for Mini Chess Game\n",
        "This module implements Value Iteration and Policy Iteration for a simplified chess endgame.\n",
        "\n",
        "Author: DRL Assignment Solution\n",
        "Date: December 2025\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, List, Dict, Set\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "\n",
        "class MiniChessEnv:\n",
        "    \"\"\"\n",
        "    Custom Mini Chess Environment for reinforcement learning.\n",
        "    White: King + Pawn vs Black: King\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, board_size: int = 4, student_id_last_digit: int = 0):\n",
        "        \"\"\"\n",
        "        Initialize the Mini Chess environment.\n",
        "\n",
        "        Args:\n",
        "            board_size: Size of the square board (4 or 5)\n",
        "            student_id_last_digit: Last digit of student ID for initial configuration\n",
        "        \"\"\"\n",
        "        self.board_size = board_size\n",
        "        self.student_id_last_digit = student_id_last_digit\n",
        "\n",
        "        self.WK = 'WK'\n",
        "        self.WP = 'WP'\n",
        "        self.BK = 'BK'\n",
        "        self.EMPTY = '.'\n",
        "\n",
        "        self.WHITE = 0\n",
        "        self.BLACK = 1\n",
        "\n",
        "        self.MOVE_LIMIT = 30\n",
        "\n",
        "        self.board = None\n",
        "        self.wk_pos = None\n",
        "        self.wp_pos = None\n",
        "        self.bk_pos = None\n",
        "        self.turn = None\n",
        "        self.move_count = 0\n",
        "        self.pawn_promoted = False\n",
        "\n",
        "    def reset(self) -> Tuple:\n",
        "        \"\"\"\n",
        "        Reset the environment to initial configuration based on student ID.\n",
        "\n",
        "        Returns:\n",
        "            Initial state tuple\n",
        "        \"\"\"\n",
        "        self.board = [[self.EMPTY for _ in range(self.board_size)] for _ in range(self.board_size)]\n",
        "        self.move_count = 0\n",
        "        self.pawn_promoted = False\n",
        "        self.turn = self.WHITE\n",
        "\n",
        "        if self.student_id_last_digit <= 4:\n",
        "            self.wk_pos = (0, 0)\n",
        "            self.bk_pos = (self.board_size - 1, self.board_size - 1)\n",
        "            self.wp_pos = (1, 0)\n",
        "        else:\n",
        "            mid = self.board_size // 2\n",
        "            self.wk_pos = (mid, mid)\n",
        "            self.bk_pos = (0, self.board_size - 1)\n",
        "            self.wp_pos = (mid + 1, mid)\n",
        "\n",
        "        self._update_board()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _update_board(self):\n",
        "        \"\"\"Update board representation from piece positions.\"\"\"\n",
        "        self.board = [[self.EMPTY for _ in range(self.board_size)] for _ in range(self.board_size)]\n",
        "\n",
        "        if self.wk_pos:\n",
        "            self.board[self.wk_pos[0]][self.wk_pos[1]] = self.WK\n",
        "        if self.wp_pos and not self.pawn_promoted:\n",
        "            self.board[self.wp_pos[0]][self.wp_pos[1]] = self.WP\n",
        "        if self.bk_pos:\n",
        "            self.board[self.bk_pos[0]][self.bk_pos[1]] = self.BK\n",
        "\n",
        "    def _get_state(self) -> Tuple:\n",
        "        \"\"\"\n",
        "        Get current state representation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: (wk_row, wk_col, wp_row, wp_col, bk_row, bk_col, turn, promoted, move_count)\n",
        "        \"\"\"\n",
        "        wp_r, wp_c = self.wp_pos if self.wp_pos else (-1, -1)\n",
        "        return (\n",
        "            self.wk_pos[0], self.wk_pos[1],\n",
        "            wp_r, wp_c,\n",
        "            self.bk_pos[0], self.bk_pos[1],\n",
        "            self.turn,\n",
        "            int(self.pawn_promoted),\n",
        "            self.move_count\n",
        "        )\n",
        "\n",
        "    def _is_valid_pos(self, pos: Tuple[int, int]) -> bool:\n",
        "        \"\"\"Check if position is within board boundaries.\"\"\"\n",
        "        r, c = pos\n",
        "        return 0 <= r < self.board_size and 0 <= c < self.board_size\n",
        "\n",
        "    def _get_king_moves(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Get all possible king moves from a position.\n",
        "\n",
        "        Args:\n",
        "            pos: King position\n",
        "\n",
        "        Returns:\n",
        "            List of valid move positions\n",
        "        \"\"\"\n",
        "        r, c = pos\n",
        "        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
        "        moves = []\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            new_pos = (r + dr, c + dc)\n",
        "            if self._is_valid_pos(new_pos):\n",
        "                moves.append(new_pos)\n",
        "\n",
        "        return moves\n",
        "\n",
        "    def _get_pawn_moves(self) -> List[Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Get all possible pawn moves (forward and diagonal captures).\n",
        "\n",
        "        Returns:\n",
        "            List of valid move positions\n",
        "        \"\"\"\n",
        "        if not self.wp_pos or self.pawn_promoted:\n",
        "            return []\n",
        "\n",
        "        r, c = self.wp_pos\n",
        "        moves = []\n",
        "\n",
        "        forward = (r + 1, c)\n",
        "        if self._is_valid_pos(forward) and self.board[forward[0]][forward[1]] == self.EMPTY:\n",
        "            moves.append(forward)\n",
        "\n",
        "        for dc in [-1, 1]:\n",
        "            capture_pos = (r + 1, c + dc)\n",
        "            if self._is_valid_pos(capture_pos):\n",
        "                if capture_pos == self.bk_pos:\n",
        "                    moves.append(capture_pos)\n",
        "\n",
        "        return moves\n",
        "\n",
        "    def _is_king_adjacent(self, pos1: Tuple[int, int], pos2: Tuple[int, int]) -> bool:\n",
        "        \"\"\"Check if two positions are adjacent (kings cannot be adjacent).\"\"\"\n",
        "        return abs(pos1[0] - pos2[0]) <= 1 and abs(pos1[1] - pos2[1]) <= 1\n",
        "\n",
        "    def _is_in_check(self, king_pos: Tuple[int, int], color: int) -> bool:\n",
        "        \"\"\"\n",
        "        Check if a king is in check.\n",
        "\n",
        "        Args:\n",
        "            king_pos: King position\n",
        "            color: King color (WHITE or BLACK)\n",
        "\n",
        "        Returns:\n",
        "            True if king is in check\n",
        "        \"\"\"\n",
        "        if color == self.BLACK:\n",
        "            if self._is_king_adjacent(king_pos, self.wk_pos):\n",
        "                return True\n",
        "\n",
        "            if self.wp_pos and not self.pawn_promoted:\n",
        "                wp_r, wp_c = self.wp_pos\n",
        "                for dc in [-1, 1]:\n",
        "                    attack_pos = (wp_r + 1, wp_c + dc)\n",
        "                    if attack_pos == king_pos:\n",
        "                        return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_legal_actions(self, state: Tuple = None) -> List[Tuple[str, Tuple[int, int]]]:\n",
        "        \"\"\"\n",
        "        Get all legal actions from current state.\n",
        "\n",
        "        Args:\n",
        "            state: State tuple (if None, uses current state)\n",
        "\n",
        "        Returns:\n",
        "            List of (piece, new_position) tuples\n",
        "        \"\"\"\n",
        "        if state:\n",
        "            self._set_state(state)\n",
        "\n",
        "        actions = []\n",
        "\n",
        "        if self.turn == self.WHITE:\n",
        "            wk_moves = self._get_king_moves(self.wk_pos)\n",
        "            for move in wk_moves:\n",
        "                if move != self.bk_pos and not self._is_king_adjacent(move, self.bk_pos):\n",
        "                    if move != self.wp_pos or self.pawn_promoted:\n",
        "                        actions.append(('WK', move))\n",
        "\n",
        "            wp_moves = self._get_pawn_moves()\n",
        "            for move in wp_moves:\n",
        "                if move != self.wk_pos:\n",
        "                    actions.append(('WP', move))\n",
        "\n",
        "        else:\n",
        "            bk_moves = self._get_king_moves(self.bk_pos)\n",
        "            for move in bk_moves:\n",
        "                if move != self.wk_pos and not self._is_king_adjacent(move, self.wk_pos):\n",
        "                    if not self.wp_pos or move != self.wp_pos or self.pawn_promoted:\n",
        "                        if not self._is_in_check(move, self.BLACK):\n",
        "                            actions.append(('BK', move))\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def _set_state(self, state: Tuple):\n",
        "        \"\"\"Set environment to a specific state.\"\"\"\n",
        "        wk_r, wk_c, wp_r, wp_c, bk_r, bk_c, turn, promoted, move_count = state\n",
        "        self.wk_pos = (wk_r, wk_c)\n",
        "        self.wp_pos = (wp_r, wp_c) if wp_r >= 0 else None\n",
        "        self.bk_pos = (bk_r, bk_c)\n",
        "        self.turn = turn\n",
        "        self.pawn_promoted = bool(promoted)\n",
        "        self.move_count = move_count\n",
        "        self._update_board()\n",
        "\n",
        "    def step(self, action: Tuple[str, Tuple[int, int]]) -> Tuple:\n",
        "        \"\"\"\n",
        "        Execute an action and return new state, reward, done.\n",
        "\n",
        "        Args:\n",
        "            action: (piece, new_position) tuple\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (new_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        piece, new_pos = action\n",
        "        reward = 0\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        if piece == 'WK':\n",
        "            self.wk_pos = new_pos\n",
        "        elif piece == 'WP':\n",
        "            if new_pos == self.bk_pos:\n",
        "                self.bk_pos = None\n",
        "                reward = -10\n",
        "                done = True\n",
        "                info['termination'] = 'pawn_captured_king'\n",
        "            else:\n",
        "                self.wp_pos = new_pos\n",
        "                if new_pos[0] == self.board_size - 1:\n",
        "                    self.pawn_promoted = True\n",
        "                    reward = 10\n",
        "                    done = True\n",
        "                    info['termination'] = 'pawn_promotion'\n",
        "        elif piece == 'BK':\n",
        "            if new_pos == self.wp_pos and not self.pawn_promoted:\n",
        "                self.wp_pos = None\n",
        "                reward = -10\n",
        "                done = True\n",
        "                info['termination'] = 'pawn_captured'\n",
        "            else:\n",
        "                self.bk_pos = new_pos\n",
        "\n",
        "        self._update_board()\n",
        "        self.move_count += 1\n",
        "\n",
        "        if not done and self.bk_pos:\n",
        "            if self._is_checkmate():\n",
        "                reward = 10\n",
        "                done = True\n",
        "                info['termination'] = 'checkmate'\n",
        "            elif self._is_stalemate():\n",
        "                reward = 0\n",
        "                done = True\n",
        "                info['termination'] = 'stalemate'\n",
        "\n",
        "        if not done and self.move_count >= self.MOVE_LIMIT:\n",
        "            reward = 0\n",
        "            done = True\n",
        "            info['termination'] = 'move_limit'\n",
        "\n",
        "        self.turn = 1 - self.turn\n",
        "\n",
        "        return self._get_state(), reward, done, info\n",
        "\n",
        "    def _is_checkmate(self) -> bool:\n",
        "        \"\"\"Check if current position is checkmate.\"\"\"\n",
        "        if self.turn == self.BLACK and self.bk_pos:\n",
        "            if self._is_in_check(self.bk_pos, self.BLACK):\n",
        "                legal_actions = self.get_legal_actions()\n",
        "                return len(legal_actions) == 0\n",
        "        return False\n",
        "\n",
        "    def _is_stalemate(self) -> bool:\n",
        "        \"\"\"Check if current position is stalemate.\"\"\"\n",
        "        if self.bk_pos:\n",
        "            if not self._is_in_check(self.bk_pos, self.BLACK):\n",
        "                legal_actions = self.get_legal_actions()\n",
        "                return len(legal_actions) == 0\n",
        "        return False\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Print current board state.\"\"\"\n",
        "        print(f\"\\nMove {self.move_count}, Turn: {'White' if self.turn == self.WHITE else 'Black'}\")\n",
        "        print(\"  \" + \" \".join([str(i) for i in range(self.board_size)]))\n",
        "        for i, row in enumerate(self.board):\n",
        "            print(f\"{i} \" + \" \".join(row))\n",
        "        print()\n",
        "\n",
        "\n",
        "class ValueIteration:\n",
        "    \"\"\"\n",
        "    Value Iteration algorithm for solving the Mini Chess MDP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: MiniChessEnv, gamma: float = 0.99, theta: float = 1e-3):\n",
        "        \"\"\"\n",
        "        Initialize Value Iteration.\n",
        "\n",
        "        Args:\n",
        "            env: Mini Chess environment\n",
        "            gamma: Discount factor\n",
        "            theta: Convergence threshold\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.theta = theta\n",
        "        self.V = {}\n",
        "        self.policy = {}\n",
        "        self.states = set()\n",
        "        self.iterations = 0\n",
        "        self.delta_history = []\n",
        "\n",
        "    def enumerate_states(self, initial_state: Tuple, max_states: int = 10000) -> Set[Tuple]:\n",
        "        \"\"\"\n",
        "        Enumerate reachable states using BFS.\n",
        "\n",
        "        Args:\n",
        "            initial_state: Starting state\n",
        "            max_states: Maximum number of states to enumerate\n",
        "\n",
        "        Returns:\n",
        "            Set of reachable states\n",
        "        \"\"\"\n",
        "        print(f\"Enumerating reachable states from initial configuration...\")\n",
        "\n",
        "        visited = set()\n",
        "        queue = deque([initial_state])\n",
        "        visited.add(initial_state)\n",
        "\n",
        "        while queue and len(visited) < max_states:\n",
        "            state = queue.popleft()\n",
        "            self.env._set_state(state)\n",
        "\n",
        "            actions = self.env.get_legal_actions()\n",
        "            for action in actions:\n",
        "                self.env._set_state(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    visited.add(next_state)\n",
        "                    if not done:\n",
        "                        queue.append(next_state)\n",
        "\n",
        "        print(f\"Enumerated {len(visited)} reachable states\")\n",
        "        self.states = visited\n",
        "        return visited\n",
        "\n",
        "    def solve(self) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"\n",
        "        Run Value Iteration algorithm.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (value_function, policy)\n",
        "        \"\"\"\n",
        "        print(f\"\\nRunning Value Iteration...\")\n",
        "        print(f\"Gamma: {self.gamma}, Theta: {self.theta}\")\n",
        "\n",
        "        for state in self.states:\n",
        "            self.V[state] = 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        self.iterations = 0\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            self.iterations += 1\n",
        "\n",
        "            for state in self.states:\n",
        "                v = self.V[state]\n",
        "                self.env._set_state(state)\n",
        "\n",
        "                actions = self.env.get_legal_actions()\n",
        "                if not actions:\n",
        "                    self.V[state] = 0.0\n",
        "                    continue\n",
        "\n",
        "                action_values = []\n",
        "                for action in actions:\n",
        "                    self.env._set_state(state)\n",
        "                    next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                    if done:\n",
        "                        value = reward\n",
        "                    else:\n",
        "                        value = reward + self.gamma * self.V.get(next_state, 0.0)\n",
        "\n",
        "                    action_values.append(value)\n",
        "\n",
        "                self.V[state] = max(action_values) if action_values else 0.0\n",
        "                delta = max(delta, abs(v - self.V[state]))\n",
        "\n",
        "            self.delta_history.append(delta)\n",
        "\n",
        "            if self.iterations % 10 == 0:\n",
        "                print(f\"  Iteration {self.iterations}: max delta = {delta:.6f}\")\n",
        "\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "        runtime = time.time() - start_time\n",
        "\n",
        "        print(f\"\\nValue Iteration Converged!\")\n",
        "        print(f\"  Iterations: {self.iterations}\")\n",
        "        print(f\"  Final max delta: {delta:.6f}\")\n",
        "        print(f\"  Runtime: {runtime:.2f} seconds\")\n",
        "\n",
        "        self._extract_policy()\n",
        "\n",
        "        return self.V, self.policy\n",
        "\n",
        "    def _extract_policy(self):\n",
        "        \"\"\"Extract greedy policy from value function.\"\"\"\n",
        "        for state in self.states:\n",
        "            self.env._set_state(state)\n",
        "            actions = self.env.get_legal_actions()\n",
        "\n",
        "            if not actions:\n",
        "                continue\n",
        "\n",
        "            action_values = []\n",
        "            for action in actions:\n",
        "                self.env._set_state(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                if done:\n",
        "                    value = reward\n",
        "                else:\n",
        "                    value = reward + self.gamma * self.V.get(next_state, 0.0)\n",
        "\n",
        "                action_values.append((action, value))\n",
        "\n",
        "            if action_values:\n",
        "                best_action = max(action_values, key=lambda x: x[1])[0]\n",
        "                self.policy[state] = best_action\n",
        "\n",
        "\n",
        "class PolicyIteration:\n",
        "    \"\"\"\n",
        "    Policy Iteration algorithm for solving the Mini Chess MDP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: MiniChessEnv, gamma: float = 0.99, theta: float = 1e-3):\n",
        "        \"\"\"\n",
        "        Initialize Policy Iteration.\n",
        "\n",
        "        Args:\n",
        "            env: Mini Chess environment\n",
        "            gamma: Discount factor\n",
        "            theta: Convergence threshold for policy evaluation\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.theta = theta\n",
        "        self.V = {}\n",
        "        self.policy = {}\n",
        "        self.states = set()\n",
        "        self.iterations = 0\n",
        "\n",
        "    def enumerate_states(self, initial_state: Tuple, max_states: int = 10000) -> Set[Tuple]:\n",
        "        \"\"\"\n",
        "        Enumerate reachable states using BFS.\n",
        "\n",
        "        Args:\n",
        "            initial_state: Starting state\n",
        "            max_states: Maximum number of states to enumerate\n",
        "\n",
        "        Returns:\n",
        "            Set of reachable states\n",
        "        \"\"\"\n",
        "        print(f\"Enumerating reachable states from initial configuration...\")\n",
        "\n",
        "        visited = set()\n",
        "        queue = deque([initial_state])\n",
        "        visited.add(initial_state)\n",
        "\n",
        "        while queue and len(visited) < max_states:\n",
        "            state = queue.popleft()\n",
        "            self.env._set_state(state)\n",
        "\n",
        "            actions = self.env.get_legal_actions()\n",
        "            for action in actions:\n",
        "                self.env._set_state(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                if next_state not in visited:\n",
        "                    visited.add(next_state)\n",
        "                    if not done:\n",
        "                        queue.append(next_state)\n",
        "\n",
        "        print(f\"Enumerated {len(visited)} reachable states\")\n",
        "        self.states = visited\n",
        "        return visited\n",
        "\n",
        "    def solve(self) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"\n",
        "        Run Policy Iteration algorithm.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (value_function, policy)\n",
        "        \"\"\"\n",
        "        print(f\"\\nRunning Policy Iteration...\")\n",
        "        print(f\"Gamma: {self.gamma}, Theta: {self.theta}\")\n",
        "\n",
        "        for state in self.states:\n",
        "            self.V[state] = 0.0\n",
        "            self.env._set_state(state)\n",
        "            actions = self.env.get_legal_actions()\n",
        "            if actions:\n",
        "                self.policy[state] = actions[0]\n",
        "\n",
        "        start_time = time.time()\n",
        "        self.iterations = 0\n",
        "\n",
        "        while True:\n",
        "            self.iterations += 1\n",
        "            print(f\"\\n  Policy Iteration - Iteration {self.iterations}\")\n",
        "\n",
        "            self._policy_evaluation()\n",
        "\n",
        "            policy_stable = self._policy_improvement()\n",
        "\n",
        "            if policy_stable:\n",
        "                break\n",
        "\n",
        "        runtime = time.time() - start_time\n",
        "\n",
        "        print(f\"\\nPolicy Iteration Converged!\")\n",
        "        print(f\"  Iterations: {self.iterations}\")\n",
        "        print(f\"  Runtime: {runtime:.2f} seconds\")\n",
        "\n",
        "        return self.V, self.policy\n",
        "\n",
        "    def _policy_evaluation(self):\n",
        "        \"\"\"Evaluate current policy.\"\"\"\n",
        "        eval_iterations = 0\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            eval_iterations += 1\n",
        "\n",
        "            for state in self.states:\n",
        "                v = self.V[state]\n",
        "\n",
        "                if state not in self.policy:\n",
        "                    continue\n",
        "\n",
        "                action = self.policy[state]\n",
        "                self.env._set_state(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                if done:\n",
        "                    self.V[state] = reward\n",
        "                else:\n",
        "                    self.V[state] = reward + self.gamma * self.V.get(next_state, 0.0)\n",
        "\n",
        "                delta = max(delta, abs(v - self.V[state]))\n",
        "\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "        print(f\"    Policy evaluation completed in {eval_iterations} iterations\")\n",
        "\n",
        "    def _policy_improvement(self) -> bool:\n",
        "        \"\"\"\n",
        "        Improve policy based on value function.\n",
        "\n",
        "        Returns:\n",
        "            True if policy is stable\n",
        "        \"\"\"\n",
        "        policy_stable = True\n",
        "\n",
        "        for state in self.states:\n",
        "            old_action = self.policy.get(state)\n",
        "\n",
        "            self.env._set_state(state)\n",
        "            actions = self.env.get_legal_actions()\n",
        "\n",
        "            if not actions:\n",
        "                continue\n",
        "\n",
        "            action_values = []\n",
        "            for action in actions:\n",
        "                self.env._set_state(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                if done:\n",
        "                    value = reward\n",
        "                else:\n",
        "                    value = reward + self.gamma * self.V.get(next_state, 0.0)\n",
        "\n",
        "                action_values.append((action, value))\n",
        "\n",
        "            if action_values:\n",
        "                best_action = max(action_values, key=lambda x: x[1])[0]\n",
        "                self.policy[state] = best_action\n",
        "\n",
        "                if old_action != best_action:\n",
        "                    policy_stable = False\n",
        "\n",
        "        return policy_stable\n",
        "\n",
        "\n",
        "def visualize_value_function(env: MiniChessEnv, V: Dict, wp_pos: Tuple, bk_pos: Tuple, title: str):\n",
        "    \"\"\"\n",
        "    Visualize value function as heatmap for different white king positions.\n",
        "\n",
        "    Args:\n",
        "        env: Mini Chess environment\n",
        "        V: Value function dictionary\n",
        "        wp_pos: Fixed white pawn position\n",
        "        bk_pos: Fixed black king position\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    board_size = env.board_size\n",
        "    value_grid = np.zeros((board_size, board_size))\n",
        "\n",
        "    for r in range(board_size):\n",
        "        for c in range(board_size):\n",
        "            wk_pos = (r, c)\n",
        "\n",
        "            if wk_pos == wp_pos or wk_pos == bk_pos:\n",
        "                value_grid[r, c] = np.nan\n",
        "                continue\n",
        "\n",
        "            state = (wk_pos[0], wk_pos[1], wp_pos[0], wp_pos[1],\n",
        "                    bk_pos[0], bk_pos[1], env.WHITE, 0, 0)\n",
        "\n",
        "            if state in V:\n",
        "                value_grid[r, c] = V[state]\n",
        "            else:\n",
        "                value_grid[r, c] = 0\n",
        "\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    sns.heatmap(value_grid, annot=True, fmt='.2f', cmap='RdYlGn',\n",
        "                center=0, cbar_kws={'label': 'State Value'},\n",
        "                xticklabels=range(board_size), yticklabels=range(board_size))\n",
        "    plt.title(f'{title}\\nWP at {wp_pos}, BK at {bk_pos}', fontweight='bold')\n",
        "    plt.xlabel('Column')\n",
        "    plt.ylabel('Row')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def demonstrate_policy(env: MiniChessEnv, policy: Dict, initial_state: Tuple, max_moves: int = 15):\n",
        "    \"\"\"\n",
        "    Demonstrate learned policy from a starting state.\n",
        "\n",
        "    Args:\n",
        "        env: Mini Chess environment\n",
        "        policy: Learned policy\n",
        "        initial_state: Starting state\n",
        "        max_moves: Maximum moves to demonstrate\n",
        "    \"\"\"\n",
        "    print(f\"\\nDemonstrating policy from initial state:\")\n",
        "    env._set_state(initial_state)\n",
        "    env.render()\n",
        "\n",
        "    state = initial_state\n",
        "    moves = 0\n",
        "\n",
        "    while moves < max_moves:\n",
        "        if state not in policy:\n",
        "            print(\"State not in policy!\")\n",
        "            break\n",
        "\n",
        "        action = policy[state]\n",
        "        piece, new_pos = action\n",
        "\n",
        "        print(f\"Move {moves + 1}: {piece} to {new_pos}\")\n",
        "\n",
        "        env._set_state(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "\n",
        "        if done:\n",
        "            print(f\"Game ended: {info.get('termination', 'unknown')}\")\n",
        "            print(f\"Reward: {reward}\")\n",
        "            break\n",
        "\n",
        "        state = next_state\n",
        "        moves += 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run Dynamic Programming solution.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"PART 2: DYNAMIC PROGRAMMING FOR MINI CHESS GAME\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "\n",
        "    student_id_last_digit = 0\n",
        "    board_size = 4 if student_id_last_digit % 2 == 0 else 5\n",
        "\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"  Student ID Last Digit: {student_id_last_digit}\")\n",
        "    print(f\"  Board Size: {board_size}x{board_size}\")\n",
        "    print()\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"1. CUSTOM MINI CHESS ENVIRONMENT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    env = MiniChessEnv(board_size=board_size, student_id_last_digit=student_id_last_digit)\n",
        "    initial_state = env.reset()\n",
        "\n",
        "    print(\"\\nInitial Configuration:\")\n",
        "    env.render()\n",
        "\n",
        "    print(f\"State Representation: {initial_state}\")\n",
        "    print(f\"Legal Actions: {len(env.get_legal_actions())}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"2. DYNAMIC PROGRAMMING - VALUE ITERATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    vi = ValueIteration(env, gamma=0.99, theta=1e-3)\n",
        "    vi.enumerate_states(initial_state, max_states=5000)\n",
        "    V_vi, policy_vi = vi.solve()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"3. DYNAMIC PROGRAMMING - POLICY ITERATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    pi = PolicyIteration(env, gamma=0.99, theta=1e-3)\n",
        "    pi.enumerate_states(initial_state, max_states=5000)\n",
        "    V_pi, policy_pi = pi.solve()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"4. STATE-VALUE FUNCTION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nVisualizing value functions...\")\n",
        "\n",
        "    wp_fixed = (1, 1)\n",
        "    bk_fixed = (3, 3) if board_size == 4 else (4, 4)\n",
        "\n",
        "    visualize_value_function(env, V_vi, wp_fixed, bk_fixed,\n",
        "                            \"Value Iteration - State Values for WK Positions\")\n",
        "\n",
        "    visualize_value_function(env, V_pi, wp_fixed, bk_fixed,\n",
        "                            \"Policy Iteration - State Values for WK Positions\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"5. POLICY DEMONSTRATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    demonstrate_policy(env, policy_vi, initial_state, max_moves=15)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"6. ANALYSIS AND DISCUSSION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\"\"\n",
        "CURSE OF DIMENSIONALITY:\n",
        "\n",
        "Current state space: ~{} states\n",
        "- Board: {}x{}\n",
        "- Pieces: 3 (WK, WP, BK)\n",
        "- State dimensions: (wk_r, wk_c, wp_r, wp_c, bk_r, bk_c, turn, promoted, moves)\n",
        "\n",
        "If we increase to 8x8 board:\n",
        "- Each piece has 64 possible positions\n",
        "- Approximate state space: 64^3 * 2 * 2 * 30 = ~15.7 million states\n",
        "- With 2 pawns: 64^4 * 2^2 * 30 = ~1 billion states\n",
        "\n",
        "If we add a rook:\n",
        "- State space grows by factor of 64\n",
        "- Dynamic programming becomes intractable\n",
        "\n",
        "IS DYNAMIC PROGRAMMING ENOUGH FOR FULL CHESS?\n",
        "\n",
        "No, standard DP is NOT tractable for full chess:\n",
        "\n",
        "1. State Space Explosion:\n",
        "   - Full chess: ~10^43 reachable positions\n",
        "   - Cannot enumerate all states\n",
        "   - Cannot store value function in memory\n",
        "\n",
        "2. Modern RL Solutions:\n",
        "   - Function Approximation: Use neural networks to approximate V(s)\n",
        "   - Monte Carlo Tree Search: Sample promising paths\n",
        "   - AlphaZero approach: Combine deep learning with MCTS\n",
        "   - Self-play reinforcement learning\n",
        "\n",
        "3. Key Insights from This Exercise:\n",
        "   - DP works perfectly for small, tractable problems\n",
        "   - Value/Policy iteration provide exact solutions\n",
        "   - Real-world problems need approximation methods\n",
        "   - Understanding DP fundamentals is crucial for advanced RL\n",
        "\n",
        "STRUCTURAL PATTERNS OBSERVED:\n",
        "\n",
        "1. Values are higher when:\n",
        "   - White king is close to the pawn (protection)\n",
        "   - Black king is far from the pawn\n",
        "   - Pawn is advanced toward promotion\n",
        "\n",
        "2. Values are lower when:\n",
        "   - Black king can capture the pawn\n",
        "   - White king is far from the pawn\n",
        "   - Pawn is blocked by black king\n",
        "\n",
        "3. Convergence:\n",
        "   - Both algorithms converged quickly (~{} iterations)\n",
        "   - Policy iteration typically needs fewer iterations\n",
        "   - Value iteration is simpler to implement\n",
        "    \"\"\".format(len(vi.states), board_size, board_size, vi.iterations))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}